{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要用anaconda环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "d:\\programs\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"D:\\s-casa毕业论文-工程\\R5R_TEST1_london\\Test2_onmypoi_data\\mergeResult\\housing_accessibility_remove_outliers.csv\")\n",
    "\n",
    "# Perform one-hot encoding on the categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=['property_type', 'borough'])\n",
    "\n",
    "# Define predictors and target variable\n",
    "X = df_encoded.drop(['price', 'address', 'lat', 'lon', 'Price_per_square_meter', 'id','Unnamed: 0'], axis=1)\n",
    "y = df_encoded['Price_per_square_meter']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[r_squared])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>size_square_meters</th>\n",
       "      <th>AccommodationAccess15m</th>\n",
       "      <th>AirportsAccess15m</th>\n",
       "      <th>Culture and tourismAccess15m</th>\n",
       "      <th>Eating and drinkingAccess15m</th>\n",
       "      <th>EducationAccess15m</th>\n",
       "      <th>EntertainmentAccess15m</th>\n",
       "      <th>HealthAccess15m</th>\n",
       "      <th>...</th>\n",
       "      <th>borough_Merton</th>\n",
       "      <th>borough_Newham</th>\n",
       "      <th>borough_Redbridge</th>\n",
       "      <th>borough_Richmond upon Thames</th>\n",
       "      <th>borough_Southwark</th>\n",
       "      <th>borough_Sutton</th>\n",
       "      <th>borough_Tower Hamlets</th>\n",
       "      <th>borough_Waltham Forest</th>\n",
       "      <th>borough_Wandsworth</th>\n",
       "      <th>borough_Westminster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bedrooms  bathrooms  size_square_meters  AccommodationAccess15m  \\\n",
       "2704       4.0        2.0               116.0                     0.0   \n",
       "871        2.0        1.0                88.0                     1.0   \n",
       "736        5.0        3.0               159.0                     2.0   \n",
       "3742       4.0        2.0               144.0                     1.0   \n",
       "1728       2.0        2.0                64.0                    10.0   \n",
       "...        ...        ...                 ...                     ...   \n",
       "1130       2.0        2.0                73.0                     1.0   \n",
       "1294       5.0        5.0               390.0                     3.0   \n",
       "860        4.0        1.0                84.0                     4.0   \n",
       "3507       4.0        2.0               131.0                     0.0   \n",
       "3174       1.0        1.0                92.0                     3.0   \n",
       "\n",
       "      AirportsAccess15m  Culture and tourismAccess15m  \\\n",
       "2704                0.0                           7.0   \n",
       "871                 0.0                           5.0   \n",
       "736                 0.0                           4.0   \n",
       "3742                0.0                          29.0   \n",
       "1728                0.0                          16.0   \n",
       "...                 ...                           ...   \n",
       "1130                0.0                          20.0   \n",
       "1294                0.0                           6.0   \n",
       "860                 0.0                           8.0   \n",
       "3507                0.0                           4.0   \n",
       "3174                0.0                          28.0   \n",
       "\n",
       "      Eating and drinkingAccess15m  EducationAccess15m  \\\n",
       "2704                         147.0                20.0   \n",
       "871                           35.0                 7.0   \n",
       "736                           98.0                 7.0   \n",
       "3742                         300.0                34.0   \n",
       "1728                         409.0                70.0   \n",
       "...                            ...                 ...   \n",
       "1130                         145.0                23.0   \n",
       "1294                          78.0                18.0   \n",
       "860                          191.0                23.0   \n",
       "3507                           6.0                 4.0   \n",
       "3174                         118.0                13.0   \n",
       "\n",
       "      EntertainmentAccess15m  HealthAccess15m  ...  borough_Merton  \\\n",
       "2704                    14.0             36.0  ...             1.0   \n",
       "871                      3.0             10.0  ...             0.0   \n",
       "736                     13.0             19.0  ...             0.0   \n",
       "3742                    42.0             62.0  ...             0.0   \n",
       "1728                    53.0            135.0  ...             0.0   \n",
       "...                      ...              ...  ...             ...   \n",
       "1130                    20.0             21.0  ...             0.0   \n",
       "1294                    15.0             21.0  ...             0.0   \n",
       "860                     17.0             59.0  ...             0.0   \n",
       "3507                     1.0              5.0  ...             0.0   \n",
       "3174                    14.0             11.0  ...             0.0   \n",
       "\n",
       "      borough_Newham  borough_Redbridge  borough_Richmond upon Thames  \\\n",
       "2704             0.0                0.0                           0.0   \n",
       "871              0.0                0.0                           0.0   \n",
       "736              0.0                0.0                           0.0   \n",
       "3742             0.0                0.0                           0.0   \n",
       "1728             0.0                0.0                           0.0   \n",
       "...              ...                ...                           ...   \n",
       "1130             0.0                0.0                           0.0   \n",
       "1294             0.0                0.0                           0.0   \n",
       "860              0.0                0.0                           0.0   \n",
       "3507             0.0                0.0                           0.0   \n",
       "3174             0.0                0.0                           0.0   \n",
       "\n",
       "      borough_Southwark  borough_Sutton  borough_Tower Hamlets  \\\n",
       "2704                0.0             0.0                    0.0   \n",
       "871                 0.0             0.0                    0.0   \n",
       "736                 0.0             0.0                    0.0   \n",
       "3742                0.0             0.0                    0.0   \n",
       "1728                0.0             0.0                    0.0   \n",
       "...                 ...             ...                    ...   \n",
       "1130                0.0             0.0                    0.0   \n",
       "1294                0.0             0.0                    0.0   \n",
       "860                 0.0             0.0                    0.0   \n",
       "3507                0.0             0.0                    0.0   \n",
       "3174                1.0             0.0                    0.0   \n",
       "\n",
       "      borough_Waltham Forest  borough_Wandsworth  borough_Westminster  \n",
       "2704                     0.0                 0.0                  0.0  \n",
       "871                      0.0                 0.0                  0.0  \n",
       "736                      0.0                 0.0                  0.0  \n",
       "3742                     0.0                 1.0                  0.0  \n",
       "1728                     0.0                 0.0                  0.0  \n",
       "...                      ...                 ...                  ...  \n",
       "1130                     0.0                 0.0                  0.0  \n",
       "1294                     0.0                 0.0                  0.0  \n",
       "860                      0.0                 0.0                  0.0  \n",
       "3507                     1.0                 0.0                  0.0  \n",
       "3174                     0.0                 0.0                  0.0  \n",
       "\n",
       "[3177 rows x 76 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 50)                3900      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,951\n",
      "Trainable params: 3,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Predicting results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Getting model weights\n",
    "weights = model.get_weights()\n",
    "\n",
    "# Getting weights of each layer\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights() # list of numpy arrays\n",
    "    #print(weights)\n",
    "# Print model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 23.64\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('R²: %.2f' %(scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 10, train_loss 141497200.0\n",
      "epoch 0, step 20, train_loss 132319368.0\n",
      "epoch 0, step 30, train_loss 124818640.0\n",
      "epoch 0, step 40, train_loss 113699944.0\n",
      "epoch 0, step 50, train_loss 175321488.0\n",
      "epoch 0, step 60, train_loss 122451264.0\n",
      "epoch 0, step 70, train_loss 120367616.0\n",
      "epoch 0, step 80, train_loss 133153216.0\n",
      "epoch 0, step 90, train_loss 128772776.0\n",
      "epoch 1, step 10, train_loss 136945392.0\n",
      "epoch 1, step 20, train_loss 96902656.0\n",
      "epoch 1, step 30, train_loss 135402496.0\n",
      "epoch 1, step 40, train_loss 148750656.0\n",
      "epoch 1, step 50, train_loss 96881096.0\n",
      "epoch 1, step 60, train_loss 96404256.0\n",
      "epoch 1, step 70, train_loss 89924688.0\n",
      "epoch 1, step 80, train_loss 159725920.0\n",
      "epoch 1, step 90, train_loss 104211480.0\n",
      "epoch 2, step 10, train_loss 115342872.0\n",
      "epoch 2, step 20, train_loss 86894336.0\n",
      "epoch 2, step 30, train_loss 154068416.0\n",
      "epoch 2, step 40, train_loss 101102952.0\n",
      "epoch 2, step 50, train_loss 102629760.0\n",
      "epoch 2, step 60, train_loss 104421848.0\n",
      "epoch 2, step 70, train_loss 89074096.0\n",
      "epoch 2, step 80, train_loss 170528944.0\n",
      "epoch 2, step 90, train_loss 184428288.0\n",
      "epoch 3, step 10, train_loss 83610560.0\n",
      "epoch 3, step 20, train_loss 94623904.0\n",
      "epoch 3, step 30, train_loss 94499592.0\n",
      "epoch 3, step 40, train_loss 73700576.0\n",
      "epoch 3, step 50, train_loss 107704424.0\n",
      "epoch 3, step 60, train_loss 170395424.0\n",
      "epoch 3, step 70, train_loss 111928176.0\n",
      "epoch 3, step 80, train_loss 78263472.0\n",
      "epoch 3, step 90, train_loss 99398256.0\n",
      "epoch 4, step 10, train_loss 80836400.0\n",
      "epoch 4, step 20, train_loss 102920000.0\n",
      "epoch 4, step 30, train_loss 76582752.0\n",
      "epoch 4, step 40, train_loss 84696248.0\n",
      "epoch 4, step 50, train_loss 90653080.0\n",
      "epoch 4, step 60, train_loss 98317008.0\n",
      "epoch 4, step 70, train_loss 87323320.0\n",
      "epoch 4, step 80, train_loss 61542248.0\n",
      "epoch 4, step 90, train_loss 110337840.0\n",
      "epoch 5, step 10, train_loss 97931904.0\n",
      "epoch 5, step 20, train_loss 61845364.0\n",
      "epoch 5, step 30, train_loss 95578768.0\n",
      "epoch 5, step 40, train_loss 122234432.0\n",
      "epoch 5, step 50, train_loss 60998680.0\n",
      "epoch 5, step 60, train_loss 68464232.0\n",
      "epoch 5, step 70, train_loss 118810768.0\n",
      "epoch 5, step 80, train_loss 111192776.0\n",
      "epoch 5, step 90, train_loss 68955032.0\n",
      "epoch 6, step 10, train_loss 75790640.0\n",
      "epoch 6, step 20, train_loss 61335540.0\n",
      "epoch 6, step 30, train_loss 100719584.0\n",
      "epoch 6, step 40, train_loss 109753472.0\n",
      "epoch 6, step 50, train_loss 78073104.0\n",
      "epoch 6, step 60, train_loss 85180128.0\n",
      "epoch 6, step 70, train_loss 106413424.0\n",
      "epoch 6, step 80, train_loss 86009976.0\n",
      "epoch 6, step 90, train_loss 115288272.0\n",
      "epoch 7, step 10, train_loss 105912848.0\n",
      "epoch 7, step 20, train_loss 110133536.0\n",
      "epoch 7, step 30, train_loss 149979968.0\n",
      "epoch 7, step 40, train_loss 114201840.0\n",
      "epoch 7, step 50, train_loss 72101496.0\n",
      "epoch 7, step 60, train_loss 91850512.0\n",
      "epoch 7, step 70, train_loss 71475088.0\n",
      "epoch 7, step 80, train_loss 84287896.0\n",
      "epoch 7, step 90, train_loss 117057664.0\n",
      "epoch 8, step 10, train_loss 146624928.0\n",
      "epoch 8, step 20, train_loss 82429880.0\n",
      "epoch 8, step 30, train_loss 87186288.0\n",
      "epoch 8, step 40, train_loss 77260672.0\n",
      "epoch 8, step 50, train_loss 135998752.0\n",
      "epoch 8, step 60, train_loss 66496640.0\n",
      "epoch 8, step 70, train_loss 87612704.0\n",
      "epoch 8, step 80, train_loss 134389648.0\n",
      "epoch 8, step 90, train_loss 72335776.0\n",
      "epoch 9, step 10, train_loss 112257904.0\n",
      "epoch 9, step 20, train_loss 87510056.0\n",
      "epoch 9, step 30, train_loss 65403264.0\n",
      "epoch 9, step 40, train_loss 75994208.0\n",
      "epoch 9, step 50, train_loss 87170728.0\n",
      "epoch 9, step 60, train_loss 95330520.0\n",
      "epoch 9, step 70, train_loss 90649112.0\n",
      "epoch 9, step 80, train_loss 50493152.0\n",
      "epoch 9, step 90, train_loss 59798016.0\n",
      "epoch 10, step 10, train_loss 81677928.0\n",
      "epoch 10, step 20, train_loss 104135520.0\n",
      "epoch 10, step 30, train_loss 71026968.0\n",
      "epoch 10, step 40, train_loss 87010304.0\n",
      "epoch 10, step 50, train_loss 88052816.0\n",
      "epoch 10, step 60, train_loss 82815312.0\n",
      "epoch 10, step 70, train_loss 61739980.0\n",
      "epoch 10, step 80, train_loss 99580432.0\n",
      "epoch 10, step 90, train_loss 145310400.0\n",
      "epoch 11, step 10, train_loss 94831680.0\n",
      "epoch 11, step 20, train_loss 69129600.0\n",
      "epoch 11, step 30, train_loss 132217872.0\n",
      "epoch 11, step 40, train_loss 38573564.0\n",
      "epoch 11, step 50, train_loss 124494520.0\n",
      "epoch 11, step 60, train_loss 64231760.0\n",
      "epoch 11, step 70, train_loss 65004880.0\n",
      "epoch 11, step 80, train_loss 66101280.0\n",
      "epoch 11, step 90, train_loss 45687472.0\n",
      "epoch 12, step 10, train_loss 72502592.0\n",
      "epoch 12, step 20, train_loss 49729944.0\n",
      "epoch 12, step 30, train_loss 114678400.0\n",
      "epoch 12, step 40, train_loss 79993632.0\n",
      "epoch 12, step 50, train_loss 111007568.0\n",
      "epoch 12, step 60, train_loss 59348904.0\n",
      "epoch 12, step 70, train_loss 119045696.0\n",
      "epoch 12, step 80, train_loss 117559328.0\n",
      "epoch 12, step 90, train_loss 32012148.0\n",
      "epoch 13, step 10, train_loss 74837248.0\n",
      "epoch 13, step 20, train_loss 103979360.0\n",
      "epoch 13, step 30, train_loss 67246392.0\n",
      "epoch 13, step 40, train_loss 51795704.0\n",
      "epoch 13, step 50, train_loss 55372272.0\n",
      "epoch 13, step 60, train_loss 71439032.0\n",
      "epoch 13, step 70, train_loss 74036096.0\n",
      "epoch 13, step 80, train_loss 150839680.0\n",
      "epoch 13, step 90, train_loss 51964048.0\n",
      "epoch 14, step 10, train_loss 73824608.0\n",
      "epoch 14, step 20, train_loss 103741896.0\n",
      "epoch 14, step 30, train_loss 186905824.0\n",
      "epoch 14, step 40, train_loss 105070352.0\n",
      "epoch 14, step 50, train_loss 109743296.0\n",
      "epoch 14, step 60, train_loss 163768016.0\n",
      "epoch 14, step 70, train_loss 84850832.0\n",
      "epoch 14, step 80, train_loss 57211576.0\n",
      "epoch 14, step 90, train_loss 43768864.0\n",
      "epoch 15, step 10, train_loss 86529472.0\n",
      "epoch 15, step 20, train_loss 70683776.0\n",
      "epoch 15, step 30, train_loss 111390704.0\n",
      "epoch 15, step 40, train_loss 93147760.0\n",
      "epoch 15, step 50, train_loss 69510552.0\n",
      "epoch 15, step 60, train_loss 74043208.0\n",
      "epoch 15, step 70, train_loss 68196344.0\n",
      "epoch 15, step 80, train_loss 87721760.0\n",
      "epoch 15, step 90, train_loss 52821384.0\n",
      "epoch 16, step 10, train_loss 110536104.0\n",
      "epoch 16, step 20, train_loss 161616240.0\n",
      "epoch 16, step 30, train_loss 109420872.0\n",
      "epoch 16, step 40, train_loss 36163640.0\n",
      "epoch 16, step 50, train_loss 75234288.0\n",
      "epoch 16, step 60, train_loss 56721000.0\n",
      "epoch 16, step 70, train_loss 131357176.0\n",
      "epoch 16, step 80, train_loss 96757568.0\n",
      "epoch 16, step 90, train_loss 54904272.0\n",
      "epoch 17, step 10, train_loss 64390760.0\n",
      "epoch 17, step 20, train_loss 64348640.0\n",
      "epoch 17, step 30, train_loss 69440440.0\n",
      "epoch 17, step 40, train_loss 100418384.0\n",
      "epoch 17, step 50, train_loss 72291024.0\n",
      "epoch 17, step 60, train_loss 86712160.0\n",
      "epoch 17, step 70, train_loss 58683164.0\n",
      "epoch 17, step 80, train_loss 82473312.0\n",
      "epoch 17, step 90, train_loss 71775408.0\n",
      "epoch 18, step 10, train_loss 68594464.0\n",
      "epoch 18, step 20, train_loss 171720736.0\n",
      "epoch 18, step 30, train_loss 52635564.0\n",
      "epoch 18, step 40, train_loss 85654664.0\n",
      "epoch 18, step 50, train_loss 76618664.0\n",
      "epoch 18, step 60, train_loss 122252728.0\n",
      "epoch 18, step 70, train_loss 50548704.0\n",
      "epoch 18, step 80, train_loss 51977504.0\n",
      "epoch 18, step 90, train_loss 65259928.0\n",
      "epoch 19, step 10, train_loss 91444912.0\n",
      "epoch 19, step 20, train_loss 48742012.0\n",
      "epoch 19, step 30, train_loss 98180096.0\n",
      "epoch 19, step 40, train_loss 153914096.0\n",
      "epoch 19, step 50, train_loss 44533164.0\n",
      "epoch 19, step 60, train_loss 87009056.0\n",
      "epoch 19, step 70, train_loss 63447480.0\n",
      "epoch 19, step 80, train_loss 64681420.0\n",
      "epoch 19, step 90, train_loss 84910192.0\n",
      "epoch 20, step 10, train_loss 76098832.0\n",
      "epoch 20, step 20, train_loss 57989088.0\n",
      "epoch 20, step 30, train_loss 68425592.0\n",
      "epoch 20, step 40, train_loss 159134768.0\n",
      "epoch 20, step 50, train_loss 87363008.0\n",
      "epoch 20, step 60, train_loss 146857984.0\n",
      "epoch 20, step 70, train_loss 56174984.0\n",
      "epoch 20, step 80, train_loss 72477760.0\n",
      "epoch 20, step 90, train_loss 26642832.0\n",
      "epoch 21, step 10, train_loss 40771112.0\n",
      "epoch 21, step 20, train_loss 57736808.0\n",
      "epoch 21, step 30, train_loss 73540816.0\n",
      "epoch 21, step 40, train_loss 49996464.0\n",
      "epoch 21, step 50, train_loss 88054032.0\n",
      "epoch 21, step 60, train_loss 56503640.0\n",
      "epoch 21, step 70, train_loss 81838880.0\n",
      "epoch 21, step 80, train_loss 54258908.0\n",
      "epoch 21, step 90, train_loss 66536152.0\n",
      "epoch 22, step 10, train_loss 114568560.0\n",
      "epoch 22, step 20, train_loss 99314616.0\n",
      "epoch 22, step 30, train_loss 75274792.0\n",
      "epoch 22, step 40, train_loss 65172380.0\n",
      "epoch 22, step 50, train_loss 85249456.0\n",
      "epoch 22, step 60, train_loss 46414752.0\n",
      "epoch 22, step 70, train_loss 108218928.0\n",
      "epoch 22, step 80, train_loss 77385120.0\n",
      "epoch 22, step 90, train_loss 124173440.0\n",
      "epoch 23, step 10, train_loss 60989632.0\n",
      "epoch 23, step 20, train_loss 62296520.0\n",
      "epoch 23, step 30, train_loss 60644804.0\n",
      "epoch 23, step 40, train_loss 65266804.0\n",
      "epoch 23, step 50, train_loss 74037560.0\n",
      "epoch 23, step 60, train_loss 133932640.0\n",
      "epoch 23, step 70, train_loss 87957616.0\n",
      "epoch 23, step 80, train_loss 110510440.0\n",
      "epoch 23, step 90, train_loss 35140304.0\n",
      "epoch 24, step 10, train_loss 66489244.0\n",
      "epoch 24, step 20, train_loss 83389968.0\n",
      "epoch 24, step 30, train_loss 48170520.0\n",
      "epoch 24, step 40, train_loss 102327360.0\n",
      "epoch 24, step 50, train_loss 46299224.0\n",
      "epoch 24, step 60, train_loss 104963744.0\n",
      "epoch 24, step 70, train_loss 128528368.0\n",
      "epoch 24, step 80, train_loss 83624944.0\n",
      "epoch 24, step 90, train_loss 44470944.0\n",
      "epoch 25, step 10, train_loss 75239696.0\n",
      "epoch 25, step 20, train_loss 47211772.0\n",
      "epoch 25, step 30, train_loss 36793736.0\n",
      "epoch 25, step 40, train_loss 134038448.0\n",
      "epoch 25, step 50, train_loss 45559624.0\n",
      "epoch 25, step 60, train_loss 119867568.0\n",
      "epoch 25, step 70, train_loss 41770032.0\n",
      "epoch 25, step 80, train_loss 40777592.0\n",
      "epoch 25, step 90, train_loss 59738024.0\n",
      "epoch 26, step 10, train_loss 67720032.0\n",
      "epoch 26, step 20, train_loss 97634880.0\n",
      "epoch 26, step 30, train_loss 95125312.0\n",
      "epoch 26, step 40, train_loss 73683160.0\n",
      "epoch 26, step 50, train_loss 62808720.0\n",
      "epoch 26, step 60, train_loss 84615280.0\n",
      "epoch 26, step 70, train_loss 57142672.0\n",
      "epoch 26, step 80, train_loss 54492232.0\n",
      "epoch 26, step 90, train_loss 132959848.0\n",
      "epoch 27, step 10, train_loss 110660080.0\n",
      "epoch 27, step 20, train_loss 50226080.0\n",
      "epoch 27, step 30, train_loss 114941576.0\n",
      "epoch 27, step 40, train_loss 67595136.0\n",
      "epoch 27, step 50, train_loss 68278184.0\n",
      "epoch 27, step 60, train_loss 34925704.0\n",
      "epoch 27, step 70, train_loss 57677076.0\n",
      "epoch 27, step 80, train_loss 97766000.0\n",
      "epoch 27, step 90, train_loss 68224488.0\n",
      "epoch 28, step 10, train_loss 52029352.0\n",
      "epoch 28, step 20, train_loss 48739432.0\n",
      "epoch 28, step 30, train_loss 58214184.0\n",
      "epoch 28, step 40, train_loss 99557032.0\n",
      "epoch 28, step 50, train_loss 47890308.0\n",
      "epoch 28, step 60, train_loss 44612964.0\n",
      "epoch 28, step 70, train_loss 107835640.0\n",
      "epoch 28, step 80, train_loss 81986320.0\n",
      "epoch 28, step 90, train_loss 42858200.0\n",
      "epoch 29, step 10, train_loss 68790744.0\n",
      "epoch 29, step 20, train_loss 45288664.0\n",
      "epoch 29, step 30, train_loss 53114320.0\n",
      "epoch 29, step 40, train_loss 55763040.0\n",
      "epoch 29, step 50, train_loss 75176768.0\n",
      "epoch 29, step 60, train_loss 39360992.0\n",
      "epoch 29, step 70, train_loss 65065392.0\n",
      "epoch 29, step 80, train_loss 89032688.0\n",
      "epoch 29, step 90, train_loss 60014316.0\n",
      "epoch 30, step 10, train_loss 29158144.0\n",
      "epoch 30, step 20, train_loss 86753432.0\n",
      "epoch 30, step 30, train_loss 30296866.0\n",
      "epoch 30, step 40, train_loss 62854132.0\n",
      "epoch 30, step 50, train_loss 36502392.0\n",
      "epoch 30, step 60, train_loss 49121264.0\n",
      "epoch 30, step 70, train_loss 39005768.0\n",
      "epoch 30, step 80, train_loss 55309336.0\n",
      "epoch 30, step 90, train_loss 39547304.0\n",
      "epoch 31, step 10, train_loss 31713280.0\n",
      "epoch 31, step 20, train_loss 86361792.0\n",
      "epoch 31, step 30, train_loss 41533200.0\n",
      "epoch 31, step 40, train_loss 66960512.0\n",
      "epoch 31, step 50, train_loss 48968648.0\n",
      "epoch 31, step 60, train_loss 40412988.0\n",
      "epoch 31, step 70, train_loss 97337376.0\n",
      "epoch 31, step 80, train_loss 131584608.0\n",
      "epoch 31, step 90, train_loss 68671048.0\n",
      "epoch 32, step 10, train_loss 33704444.0\n",
      "epoch 32, step 20, train_loss 79648688.0\n",
      "epoch 32, step 30, train_loss 79867720.0\n",
      "epoch 32, step 40, train_loss 39496632.0\n",
      "epoch 32, step 50, train_loss 22927476.0\n",
      "epoch 32, step 60, train_loss 69433272.0\n",
      "epoch 32, step 70, train_loss 65578888.0\n",
      "epoch 32, step 80, train_loss 61322324.0\n",
      "epoch 32, step 90, train_loss 40367468.0\n",
      "epoch 33, step 10, train_loss 55082288.0\n",
      "epoch 33, step 20, train_loss 38716536.0\n",
      "epoch 33, step 30, train_loss 28301000.0\n",
      "epoch 33, step 40, train_loss 25976080.0\n",
      "epoch 33, step 50, train_loss 120178752.0\n",
      "epoch 33, step 60, train_loss 89977128.0\n",
      "epoch 33, step 70, train_loss 30436966.0\n",
      "epoch 33, step 80, train_loss 60548200.0\n",
      "epoch 33, step 90, train_loss 58219604.0\n",
      "epoch 34, step 10, train_loss 26192368.0\n",
      "epoch 34, step 20, train_loss 72566392.0\n",
      "epoch 34, step 30, train_loss 67818744.0\n",
      "epoch 34, step 40, train_loss 47505804.0\n",
      "epoch 34, step 50, train_loss 72557168.0\n",
      "epoch 34, step 60, train_loss 76723600.0\n",
      "epoch 34, step 70, train_loss 93032688.0\n",
      "epoch 34, step 80, train_loss 71725520.0\n",
      "epoch 34, step 90, train_loss 57684144.0\n",
      "epoch 35, step 10, train_loss 36396104.0\n",
      "epoch 35, step 20, train_loss 89415192.0\n",
      "epoch 35, step 30, train_loss 56876260.0\n",
      "epoch 35, step 40, train_loss 58259528.0\n",
      "epoch 35, step 50, train_loss 26175710.0\n",
      "epoch 35, step 60, train_loss 26587556.0\n",
      "epoch 35, step 70, train_loss 61972604.0\n",
      "epoch 35, step 80, train_loss 42279648.0\n",
      "epoch 35, step 90, train_loss 37669292.0\n",
      "epoch 36, step 10, train_loss 50776280.0\n",
      "epoch 36, step 20, train_loss 90166800.0\n",
      "epoch 36, step 30, train_loss 76416976.0\n",
      "epoch 36, step 40, train_loss 77137544.0\n",
      "epoch 36, step 50, train_loss 42518160.0\n",
      "epoch 36, step 60, train_loss 71206160.0\n",
      "epoch 36, step 70, train_loss 35502456.0\n",
      "epoch 36, step 80, train_loss 46063540.0\n",
      "epoch 36, step 90, train_loss 50861884.0\n",
      "epoch 37, step 10, train_loss 60877076.0\n",
      "epoch 37, step 20, train_loss 43394624.0\n",
      "epoch 37, step 30, train_loss 124535392.0\n",
      "epoch 37, step 40, train_loss 91010800.0\n",
      "epoch 37, step 50, train_loss 55597684.0\n",
      "epoch 37, step 60, train_loss 53396592.0\n",
      "epoch 37, step 70, train_loss 50411136.0\n",
      "epoch 37, step 80, train_loss 31593448.0\n",
      "epoch 37, step 90, train_loss 52989728.0\n",
      "epoch 38, step 10, train_loss 59006388.0\n",
      "epoch 38, step 20, train_loss 37180200.0\n",
      "epoch 38, step 30, train_loss 57078184.0\n",
      "epoch 38, step 40, train_loss 62601888.0\n",
      "epoch 38, step 50, train_loss 57372144.0\n",
      "epoch 38, step 60, train_loss 46292256.0\n",
      "epoch 38, step 70, train_loss 58669884.0\n",
      "epoch 38, step 80, train_loss 44386472.0\n",
      "epoch 38, step 90, train_loss 31822316.0\n",
      "epoch 39, step 10, train_loss 97554240.0\n",
      "epoch 39, step 20, train_loss 33324600.0\n",
      "epoch 39, step 30, train_loss 64418824.0\n",
      "epoch 39, step 40, train_loss 71853616.0\n",
      "epoch 39, step 50, train_loss 78981144.0\n",
      "epoch 39, step 60, train_loss 24290918.0\n",
      "epoch 39, step 70, train_loss 28165444.0\n",
      "epoch 39, step 80, train_loss 45031156.0\n",
      "epoch 39, step 90, train_loss 63257620.0\n",
      "epoch 40, step 10, train_loss 55757692.0\n",
      "epoch 40, step 20, train_loss 90682272.0\n",
      "epoch 40, step 30, train_loss 68037888.0\n",
      "epoch 40, step 40, train_loss 28840542.0\n",
      "epoch 40, step 50, train_loss 31242436.0\n",
      "epoch 40, step 60, train_loss 48253664.0\n",
      "epoch 40, step 70, train_loss 53628128.0\n",
      "epoch 40, step 80, train_loss 59013936.0\n",
      "epoch 40, step 90, train_loss 73491200.0\n",
      "epoch 41, step 10, train_loss 54437208.0\n",
      "epoch 41, step 20, train_loss 55559480.0\n",
      "epoch 41, step 30, train_loss 41962256.0\n",
      "epoch 41, step 40, train_loss 62488368.0\n",
      "epoch 41, step 50, train_loss 16781738.0\n",
      "epoch 41, step 60, train_loss 49700704.0\n",
      "epoch 41, step 70, train_loss 45103168.0\n",
      "epoch 41, step 80, train_loss 40257120.0\n",
      "epoch 41, step 90, train_loss 34535328.0\n",
      "epoch 42, step 10, train_loss 44984728.0\n",
      "epoch 42, step 20, train_loss 41716912.0\n",
      "epoch 42, step 30, train_loss 71666896.0\n",
      "epoch 42, step 40, train_loss 14869718.0\n",
      "epoch 42, step 50, train_loss 38473544.0\n",
      "epoch 42, step 60, train_loss 55752716.0\n",
      "epoch 42, step 70, train_loss 23229594.0\n",
      "epoch 42, step 80, train_loss 48404392.0\n",
      "epoch 42, step 90, train_loss 57521716.0\n",
      "epoch 43, step 10, train_loss 50630368.0\n",
      "epoch 43, step 20, train_loss 59257624.0\n",
      "epoch 43, step 30, train_loss 71887568.0\n",
      "epoch 43, step 40, train_loss 26695600.0\n",
      "epoch 43, step 50, train_loss 44551080.0\n",
      "epoch 43, step 60, train_loss 20956312.0\n",
      "epoch 43, step 70, train_loss 54424184.0\n",
      "epoch 43, step 80, train_loss 79078864.0\n",
      "epoch 43, step 90, train_loss 66292776.0\n",
      "epoch 44, step 10, train_loss 37808400.0\n",
      "epoch 44, step 20, train_loss 75554016.0\n",
      "epoch 44, step 30, train_loss 29019632.0\n",
      "epoch 44, step 40, train_loss 43269508.0\n",
      "epoch 44, step 50, train_loss 59813108.0\n",
      "epoch 44, step 60, train_loss 70913264.0\n",
      "epoch 44, step 70, train_loss 75825040.0\n",
      "epoch 44, step 80, train_loss 43016104.0\n",
      "epoch 44, step 90, train_loss 28615094.0\n",
      "epoch 45, step 10, train_loss 74358992.0\n",
      "epoch 45, step 20, train_loss 38069072.0\n",
      "epoch 45, step 30, train_loss 55993168.0\n",
      "epoch 45, step 40, train_loss 46560224.0\n",
      "epoch 45, step 50, train_loss 24940476.0\n",
      "epoch 45, step 60, train_loss 33494426.0\n",
      "epoch 45, step 70, train_loss 61463840.0\n",
      "epoch 45, step 80, train_loss 95343352.0\n",
      "epoch 45, step 90, train_loss 51114472.0\n",
      "epoch 46, step 10, train_loss 39202512.0\n",
      "epoch 46, step 20, train_loss 33657048.0\n",
      "epoch 46, step 30, train_loss 34174232.0\n",
      "epoch 46, step 40, train_loss 49403620.0\n",
      "epoch 46, step 50, train_loss 56118700.0\n",
      "epoch 46, step 60, train_loss 82909520.0\n",
      "epoch 46, step 70, train_loss 45402200.0\n",
      "epoch 46, step 80, train_loss 35056072.0\n",
      "epoch 46, step 90, train_loss 79270744.0\n",
      "epoch 47, step 10, train_loss 39606304.0\n",
      "epoch 47, step 20, train_loss 31431568.0\n",
      "epoch 47, step 30, train_loss 72469240.0\n",
      "epoch 47, step 40, train_loss 35378576.0\n",
      "epoch 47, step 50, train_loss 98122784.0\n",
      "epoch 47, step 60, train_loss 47006848.0\n",
      "epoch 47, step 70, train_loss 68651216.0\n",
      "epoch 47, step 80, train_loss 34642464.0\n",
      "epoch 47, step 90, train_loss 42050312.0\n",
      "epoch 48, step 10, train_loss 22171074.0\n",
      "epoch 48, step 20, train_loss 41419404.0\n",
      "epoch 48, step 30, train_loss 103013056.0\n",
      "epoch 48, step 40, train_loss 27081394.0\n",
      "epoch 48, step 50, train_loss 89728624.0\n",
      "epoch 48, step 60, train_loss 42114904.0\n",
      "epoch 48, step 70, train_loss 49865252.0\n",
      "epoch 48, step 80, train_loss 42000948.0\n",
      "epoch 48, step 90, train_loss 31070518.0\n",
      "epoch 49, step 10, train_loss 39932892.0\n",
      "epoch 49, step 20, train_loss 51155064.0\n",
      "epoch 49, step 30, train_loss 37557716.0\n",
      "epoch 49, step 40, train_loss 27973660.0\n",
      "epoch 49, step 50, train_loss 39731852.0\n",
      "epoch 49, step 60, train_loss 39272488.0\n",
      "epoch 49, step 70, train_loss 79201896.0\n",
      "epoch 49, step 80, train_loss 38797188.0\n",
      "epoch 49, step 90, train_loss 58101568.0\n",
      "epoch 50, step 10, train_loss 53268828.0\n",
      "epoch 50, step 20, train_loss 57790028.0\n",
      "epoch 50, step 30, train_loss 48271000.0\n",
      "epoch 50, step 40, train_loss 47207768.0\n",
      "epoch 50, step 50, train_loss 66673752.0\n",
      "epoch 50, step 60, train_loss 41449592.0\n",
      "epoch 50, step 70, train_loss 44512984.0\n",
      "epoch 50, step 80, train_loss 50871648.0\n",
      "epoch 50, step 90, train_loss 94917256.0\n",
      "epoch 51, step 10, train_loss 64483800.0\n",
      "epoch 51, step 20, train_loss 46362692.0\n",
      "epoch 51, step 30, train_loss 18825634.0\n",
      "epoch 51, step 40, train_loss 34470880.0\n",
      "epoch 51, step 50, train_loss 67821776.0\n",
      "epoch 51, step 60, train_loss 68064992.0\n",
      "epoch 51, step 70, train_loss 52059532.0\n",
      "epoch 51, step 80, train_loss 34372572.0\n",
      "epoch 51, step 90, train_loss 35972680.0\n",
      "epoch 52, step 10, train_loss 50891552.0\n",
      "epoch 52, step 20, train_loss 39277940.0\n",
      "epoch 52, step 30, train_loss 34695616.0\n",
      "epoch 52, step 40, train_loss 95158704.0\n",
      "epoch 52, step 50, train_loss 31249972.0\n",
      "epoch 52, step 60, train_loss 61354336.0\n",
      "epoch 52, step 70, train_loss 75150808.0\n",
      "epoch 52, step 80, train_loss 34789264.0\n",
      "epoch 52, step 90, train_loss 28416132.0\n",
      "epoch 53, step 10, train_loss 30969006.0\n",
      "epoch 53, step 20, train_loss 76353680.0\n",
      "epoch 53, step 30, train_loss 65129396.0\n",
      "epoch 53, step 40, train_loss 46700336.0\n",
      "epoch 53, step 50, train_loss 36743616.0\n",
      "epoch 53, step 60, train_loss 43086992.0\n",
      "epoch 53, step 70, train_loss 41350012.0\n",
      "epoch 53, step 80, train_loss 45018448.0\n",
      "epoch 53, step 90, train_loss 40098648.0\n",
      "epoch 54, step 10, train_loss 55392936.0\n",
      "epoch 54, step 20, train_loss 27526234.0\n",
      "epoch 54, step 30, train_loss 58673116.0\n",
      "epoch 54, step 40, train_loss 60713624.0\n",
      "epoch 54, step 50, train_loss 31045536.0\n",
      "epoch 54, step 60, train_loss 30542788.0\n",
      "epoch 54, step 70, train_loss 29341848.0\n",
      "epoch 54, step 80, train_loss 38077716.0\n",
      "epoch 54, step 90, train_loss 32349582.0\n",
      "epoch 55, step 10, train_loss 21182862.0\n",
      "epoch 55, step 20, train_loss 38138824.0\n",
      "epoch 55, step 30, train_loss 40443472.0\n",
      "epoch 55, step 40, train_loss 36661344.0\n",
      "epoch 55, step 50, train_loss 54674108.0\n",
      "epoch 55, step 60, train_loss 59980380.0\n",
      "epoch 55, step 70, train_loss 115080424.0\n",
      "epoch 55, step 80, train_loss 43076352.0\n",
      "epoch 55, step 90, train_loss 55520220.0\n",
      "epoch 56, step 10, train_loss 74739632.0\n",
      "epoch 56, step 20, train_loss 43447084.0\n",
      "epoch 56, step 30, train_loss 54390376.0\n",
      "epoch 56, step 40, train_loss 51145652.0\n",
      "epoch 56, step 50, train_loss 43002592.0\n",
      "epoch 56, step 60, train_loss 33318072.0\n",
      "epoch 56, step 70, train_loss 52043480.0\n",
      "epoch 56, step 80, train_loss 56702440.0\n",
      "epoch 56, step 90, train_loss 102578784.0\n",
      "epoch 57, step 10, train_loss 28289804.0\n",
      "epoch 57, step 20, train_loss 56046528.0\n",
      "epoch 57, step 30, train_loss 25356766.0\n",
      "epoch 57, step 40, train_loss 36820312.0\n",
      "epoch 57, step 50, train_loss 25864116.0\n",
      "epoch 57, step 60, train_loss 76748184.0\n",
      "epoch 57, step 70, train_loss 33140738.0\n",
      "epoch 57, step 80, train_loss 45312556.0\n",
      "epoch 57, step 90, train_loss 58267516.0\n",
      "epoch 58, step 10, train_loss 40042368.0\n",
      "epoch 58, step 20, train_loss 45601948.0\n",
      "epoch 58, step 30, train_loss 54627204.0\n",
      "epoch 58, step 40, train_loss 39904500.0\n",
      "epoch 58, step 50, train_loss 31216120.0\n",
      "epoch 58, step 60, train_loss 23696432.0\n",
      "epoch 58, step 70, train_loss 16627554.0\n",
      "epoch 58, step 80, train_loss 36145984.0\n",
      "epoch 58, step 90, train_loss 25182044.0\n",
      "epoch 59, step 10, train_loss 24362644.0\n",
      "epoch 59, step 20, train_loss 30962816.0\n",
      "epoch 59, step 30, train_loss 68422160.0\n",
      "epoch 59, step 40, train_loss 65236436.0\n",
      "epoch 59, step 50, train_loss 16272780.0\n",
      "epoch 59, step 60, train_loss 50111468.0\n",
      "epoch 59, step 70, train_loss 50597660.0\n",
      "epoch 59, step 80, train_loss 53179968.0\n",
      "epoch 59, step 90, train_loss 44462824.0\n",
      "epoch 60, step 10, train_loss 68176336.0\n",
      "epoch 60, step 20, train_loss 44185360.0\n",
      "epoch 60, step 30, train_loss 80083968.0\n",
      "epoch 60, step 40, train_loss 29956418.0\n",
      "epoch 60, step 50, train_loss 39526608.0\n",
      "epoch 60, step 60, train_loss 55843584.0\n",
      "epoch 60, step 70, train_loss 29988542.0\n",
      "epoch 60, step 80, train_loss 52584424.0\n",
      "epoch 60, step 90, train_loss 56150396.0\n",
      "epoch 61, step 10, train_loss 44567528.0\n",
      "epoch 61, step 20, train_loss 47622272.0\n",
      "epoch 61, step 30, train_loss 66922948.0\n",
      "epoch 61, step 40, train_loss 39821292.0\n",
      "epoch 61, step 50, train_loss 63317976.0\n",
      "epoch 61, step 60, train_loss 49400400.0\n",
      "epoch 61, step 70, train_loss 65719008.0\n",
      "epoch 61, step 80, train_loss 66647744.0\n",
      "epoch 61, step 90, train_loss 84764272.0\n",
      "epoch 62, step 10, train_loss 77483504.0\n",
      "epoch 62, step 20, train_loss 67112656.0\n",
      "epoch 62, step 30, train_loss 59415660.0\n",
      "epoch 62, step 40, train_loss 13226774.0\n",
      "epoch 62, step 50, train_loss 36791128.0\n",
      "epoch 62, step 60, train_loss 33711864.0\n",
      "epoch 62, step 70, train_loss 39533832.0\n",
      "epoch 62, step 80, train_loss 18850852.0\n",
      "epoch 62, step 90, train_loss 57047352.0\n",
      "epoch 63, step 10, train_loss 86666072.0\n",
      "epoch 63, step 20, train_loss 37920920.0\n",
      "epoch 63, step 30, train_loss 42878104.0\n",
      "epoch 63, step 40, train_loss 53715148.0\n",
      "epoch 63, step 50, train_loss 30912560.0\n",
      "epoch 63, step 60, train_loss 51516672.0\n",
      "epoch 63, step 70, train_loss 49905692.0\n",
      "epoch 63, step 80, train_loss 46056884.0\n",
      "epoch 63, step 90, train_loss 38778008.0\n",
      "epoch 64, step 10, train_loss 44707432.0\n",
      "epoch 64, step 20, train_loss 57255868.0\n",
      "epoch 64, step 30, train_loss 29337508.0\n",
      "epoch 64, step 40, train_loss 33128880.0\n",
      "epoch 64, step 50, train_loss 61000756.0\n",
      "epoch 64, step 60, train_loss 59891200.0\n",
      "epoch 64, step 70, train_loss 39364448.0\n",
      "epoch 64, step 80, train_loss 18402250.0\n",
      "epoch 64, step 90, train_loss 26753776.0\n",
      "epoch 65, step 10, train_loss 45634840.0\n",
      "epoch 65, step 20, train_loss 75267200.0\n",
      "epoch 65, step 30, train_loss 37593032.0\n",
      "epoch 65, step 40, train_loss 35740352.0\n",
      "epoch 65, step 50, train_loss 45928088.0\n",
      "epoch 65, step 60, train_loss 33047314.0\n",
      "epoch 65, step 70, train_loss 71023504.0\n",
      "epoch 65, step 80, train_loss 48207200.0\n",
      "epoch 65, step 90, train_loss 48849516.0\n",
      "epoch 66, step 10, train_loss 46271944.0\n",
      "epoch 66, step 20, train_loss 30281168.0\n",
      "epoch 66, step 30, train_loss 70549424.0\n",
      "epoch 66, step 40, train_loss 85458064.0\n",
      "epoch 66, step 50, train_loss 42367136.0\n",
      "epoch 66, step 60, train_loss 35682408.0\n",
      "epoch 66, step 70, train_loss 57840560.0\n",
      "epoch 66, step 80, train_loss 46829512.0\n",
      "epoch 66, step 90, train_loss 40363520.0\n",
      "epoch 67, step 10, train_loss 45336628.0\n",
      "epoch 67, step 20, train_loss 32699792.0\n",
      "epoch 67, step 30, train_loss 36485132.0\n",
      "epoch 67, step 40, train_loss 40360256.0\n",
      "epoch 67, step 50, train_loss 26365364.0\n",
      "epoch 67, step 60, train_loss 31002146.0\n",
      "epoch 67, step 70, train_loss 30751296.0\n",
      "epoch 67, step 80, train_loss 97787688.0\n",
      "epoch 67, step 90, train_loss 30359448.0\n",
      "epoch 68, step 10, train_loss 38666772.0\n",
      "epoch 68, step 20, train_loss 60581664.0\n",
      "epoch 68, step 30, train_loss 59245088.0\n",
      "epoch 68, step 40, train_loss 24179448.0\n",
      "epoch 68, step 50, train_loss 35541804.0\n",
      "epoch 68, step 60, train_loss 106293480.0\n",
      "epoch 68, step 70, train_loss 69519512.0\n",
      "epoch 68, step 80, train_loss 39696080.0\n",
      "epoch 68, step 90, train_loss 39044976.0\n",
      "epoch 69, step 10, train_loss 99016912.0\n",
      "epoch 69, step 20, train_loss 66225760.0\n",
      "epoch 69, step 30, train_loss 45138584.0\n",
      "epoch 69, step 40, train_loss 27783342.0\n",
      "epoch 69, step 50, train_loss 23583724.0\n",
      "epoch 69, step 60, train_loss 29707248.0\n",
      "epoch 69, step 70, train_loss 55659336.0\n",
      "epoch 69, step 80, train_loss 20361572.0\n",
      "epoch 69, step 90, train_loss 66602408.0\n",
      "epoch 70, step 10, train_loss 47453004.0\n",
      "epoch 70, step 20, train_loss 71941808.0\n",
      "epoch 70, step 30, train_loss 29818716.0\n",
      "epoch 70, step 40, train_loss 60848912.0\n",
      "epoch 70, step 50, train_loss 47441000.0\n",
      "epoch 70, step 60, train_loss 40678636.0\n",
      "epoch 70, step 70, train_loss 36213280.0\n",
      "epoch 70, step 80, train_loss 39813712.0\n",
      "epoch 70, step 90, train_loss 36233544.0\n",
      "epoch 71, step 10, train_loss 76487936.0\n",
      "epoch 71, step 20, train_loss 24371616.0\n",
      "epoch 71, step 30, train_loss 25374912.0\n",
      "epoch 71, step 40, train_loss 33484400.0\n",
      "epoch 71, step 50, train_loss 63327872.0\n",
      "epoch 71, step 60, train_loss 47230152.0\n",
      "epoch 71, step 70, train_loss 68088000.0\n",
      "epoch 71, step 80, train_loss 39733204.0\n",
      "epoch 71, step 90, train_loss 66743192.0\n",
      "epoch 72, step 10, train_loss 35546080.0\n",
      "epoch 72, step 20, train_loss 86699888.0\n",
      "epoch 72, step 30, train_loss 24070726.0\n",
      "epoch 72, step 40, train_loss 85932504.0\n",
      "epoch 72, step 50, train_loss 22454416.0\n",
      "epoch 72, step 60, train_loss 88718568.0\n",
      "epoch 72, step 70, train_loss 70651744.0\n",
      "epoch 72, step 80, train_loss 38490216.0\n",
      "epoch 72, step 90, train_loss 38854108.0\n",
      "epoch 73, step 10, train_loss 51351164.0\n",
      "epoch 73, step 20, train_loss 37970616.0\n",
      "epoch 73, step 30, train_loss 60805120.0\n",
      "epoch 73, step 40, train_loss 76196272.0\n",
      "epoch 73, step 50, train_loss 64965960.0\n",
      "epoch 73, step 60, train_loss 36628040.0\n",
      "epoch 73, step 70, train_loss 60020380.0\n",
      "epoch 73, step 80, train_loss 32203354.0\n",
      "epoch 73, step 90, train_loss 79700144.0\n",
      "epoch 74, step 10, train_loss 21591976.0\n",
      "epoch 74, step 20, train_loss 66365772.0\n",
      "epoch 74, step 30, train_loss 32662292.0\n",
      "epoch 74, step 40, train_loss 35391920.0\n",
      "epoch 74, step 50, train_loss 39531672.0\n",
      "epoch 74, step 60, train_loss 29185376.0\n",
      "epoch 74, step 70, train_loss 65891388.0\n",
      "epoch 74, step 80, train_loss 52833076.0\n",
      "epoch 74, step 90, train_loss 59560376.0\n",
      "epoch 75, step 10, train_loss 50022888.0\n",
      "epoch 75, step 20, train_loss 32283792.0\n",
      "epoch 75, step 30, train_loss 28484124.0\n",
      "epoch 75, step 40, train_loss 39215364.0\n",
      "epoch 75, step 50, train_loss 29181684.0\n",
      "epoch 75, step 60, train_loss 14726262.0\n",
      "epoch 75, step 70, train_loss 40465640.0\n",
      "epoch 75, step 80, train_loss 49948768.0\n",
      "epoch 75, step 90, train_loss 48049816.0\n",
      "epoch 76, step 10, train_loss 70052192.0\n",
      "epoch 76, step 20, train_loss 50342536.0\n",
      "epoch 76, step 30, train_loss 47722968.0\n",
      "epoch 76, step 40, train_loss 85945208.0\n",
      "epoch 76, step 50, train_loss 53332424.0\n",
      "epoch 76, step 60, train_loss 25421798.0\n",
      "epoch 76, step 70, train_loss 29610372.0\n",
      "epoch 76, step 80, train_loss 36679152.0\n",
      "epoch 76, step 90, train_loss 24861716.0\n",
      "epoch 77, step 10, train_loss 53440264.0\n",
      "epoch 77, step 20, train_loss 37785376.0\n",
      "epoch 77, step 30, train_loss 31655540.0\n",
      "epoch 77, step 40, train_loss 58696224.0\n",
      "epoch 77, step 50, train_loss 65375076.0\n",
      "epoch 77, step 60, train_loss 72420416.0\n",
      "epoch 77, step 70, train_loss 90736136.0\n",
      "epoch 77, step 80, train_loss 46357288.0\n",
      "epoch 77, step 90, train_loss 31201012.0\n",
      "epoch 78, step 10, train_loss 43893280.0\n",
      "epoch 78, step 20, train_loss 82694888.0\n",
      "epoch 78, step 30, train_loss 51302364.0\n",
      "epoch 78, step 40, train_loss 45397640.0\n",
      "epoch 78, step 50, train_loss 51587336.0\n",
      "epoch 78, step 60, train_loss 30289710.0\n",
      "epoch 78, step 70, train_loss 69534024.0\n",
      "epoch 78, step 80, train_loss 63288520.0\n",
      "epoch 78, step 90, train_loss 50162224.0\n",
      "epoch 79, step 10, train_loss 23243870.0\n",
      "epoch 79, step 20, train_loss 46605100.0\n",
      "epoch 79, step 30, train_loss 53269176.0\n",
      "epoch 79, step 40, train_loss 26209560.0\n",
      "epoch 79, step 50, train_loss 39240664.0\n",
      "epoch 79, step 60, train_loss 66431616.0\n",
      "epoch 79, step 70, train_loss 51767976.0\n",
      "epoch 79, step 80, train_loss 45638336.0\n",
      "epoch 79, step 90, train_loss 48201208.0\n",
      "epoch 80, step 10, train_loss 53438876.0\n",
      "epoch 80, step 20, train_loss 22441756.0\n",
      "epoch 80, step 30, train_loss 32464804.0\n",
      "epoch 80, step 40, train_loss 57458708.0\n",
      "epoch 80, step 50, train_loss 35042540.0\n",
      "epoch 80, step 60, train_loss 68247144.0\n",
      "epoch 80, step 70, train_loss 61404544.0\n",
      "epoch 80, step 80, train_loss 39678564.0\n",
      "epoch 80, step 90, train_loss 22266906.0\n",
      "epoch 81, step 10, train_loss 43527852.0\n",
      "epoch 81, step 20, train_loss 73407024.0\n",
      "epoch 81, step 30, train_loss 51996184.0\n",
      "epoch 81, step 40, train_loss 34762560.0\n",
      "epoch 81, step 50, train_loss 40580680.0\n",
      "epoch 81, step 60, train_loss 69506208.0\n",
      "epoch 81, step 70, train_loss 38974208.0\n",
      "epoch 81, step 80, train_loss 51679288.0\n",
      "epoch 81, step 90, train_loss 61262856.0\n",
      "epoch 82, step 10, train_loss 30820300.0\n",
      "epoch 82, step 20, train_loss 76825080.0\n",
      "epoch 82, step 30, train_loss 28770736.0\n",
      "epoch 82, step 40, train_loss 38331848.0\n",
      "epoch 82, step 50, train_loss 53102232.0\n",
      "epoch 82, step 60, train_loss 84888792.0\n",
      "epoch 82, step 70, train_loss 63956612.0\n",
      "epoch 82, step 80, train_loss 23015872.0\n",
      "epoch 82, step 90, train_loss 32010048.0\n",
      "epoch 83, step 10, train_loss 58481320.0\n",
      "epoch 83, step 20, train_loss 49765696.0\n",
      "epoch 83, step 30, train_loss 34101480.0\n",
      "epoch 83, step 40, train_loss 54800672.0\n",
      "epoch 83, step 50, train_loss 27177148.0\n",
      "epoch 83, step 60, train_loss 93369592.0\n",
      "epoch 83, step 70, train_loss 62317280.0\n",
      "epoch 83, step 80, train_loss 31131876.0\n",
      "epoch 83, step 90, train_loss 36151240.0\n",
      "epoch 84, step 10, train_loss 71328880.0\n",
      "epoch 84, step 20, train_loss 34108608.0\n",
      "epoch 84, step 30, train_loss 49544776.0\n",
      "epoch 84, step 40, train_loss 51833168.0\n",
      "epoch 84, step 50, train_loss 35824924.0\n",
      "epoch 84, step 60, train_loss 52690792.0\n",
      "epoch 84, step 70, train_loss 44212608.0\n",
      "epoch 84, step 80, train_loss 108510176.0\n",
      "epoch 84, step 90, train_loss 70901672.0\n",
      "epoch 85, step 10, train_loss 85982256.0\n",
      "epoch 85, step 20, train_loss 25771058.0\n",
      "epoch 85, step 30, train_loss 30674644.0\n",
      "epoch 85, step 40, train_loss 25902156.0\n",
      "epoch 85, step 50, train_loss 87998016.0\n",
      "epoch 85, step 60, train_loss 54219608.0\n",
      "epoch 85, step 70, train_loss 57222816.0\n",
      "epoch 85, step 80, train_loss 39141456.0\n",
      "epoch 85, step 90, train_loss 45479604.0\n",
      "epoch 86, step 10, train_loss 62708456.0\n",
      "epoch 86, step 20, train_loss 35994264.0\n",
      "epoch 86, step 30, train_loss 66445380.0\n",
      "epoch 86, step 40, train_loss 35410648.0\n",
      "epoch 86, step 50, train_loss 51342320.0\n",
      "epoch 86, step 60, train_loss 20027708.0\n",
      "epoch 86, step 70, train_loss 41186048.0\n",
      "epoch 86, step 80, train_loss 46203560.0\n",
      "epoch 86, step 90, train_loss 30566280.0\n",
      "epoch 87, step 10, train_loss 54042448.0\n",
      "epoch 87, step 20, train_loss 49529576.0\n",
      "epoch 87, step 30, train_loss 47018784.0\n",
      "epoch 87, step 40, train_loss 18774620.0\n",
      "epoch 87, step 50, train_loss 60292016.0\n",
      "epoch 87, step 60, train_loss 75296352.0\n",
      "epoch 87, step 70, train_loss 44422808.0\n",
      "epoch 87, step 80, train_loss 15552922.0\n",
      "epoch 87, step 90, train_loss 18680676.0\n",
      "epoch 88, step 10, train_loss 54586288.0\n",
      "epoch 88, step 20, train_loss 33030692.0\n",
      "epoch 88, step 30, train_loss 83178960.0\n",
      "epoch 88, step 40, train_loss 20577168.0\n",
      "epoch 88, step 50, train_loss 30256016.0\n",
      "epoch 88, step 60, train_loss 35905888.0\n",
      "epoch 88, step 70, train_loss 31689964.0\n",
      "epoch 88, step 80, train_loss 55461424.0\n",
      "epoch 88, step 90, train_loss 23355664.0\n",
      "epoch 89, step 10, train_loss 29892744.0\n",
      "epoch 89, step 20, train_loss 52087232.0\n",
      "epoch 89, step 30, train_loss 57587432.0\n",
      "epoch 89, step 40, train_loss 62996112.0\n",
      "epoch 89, step 50, train_loss 84627872.0\n",
      "epoch 89, step 60, train_loss 36959048.0\n",
      "epoch 89, step 70, train_loss 45818816.0\n",
      "epoch 89, step 80, train_loss 45948192.0\n",
      "epoch 89, step 90, train_loss 28346884.0\n",
      "epoch 90, step 10, train_loss 40320672.0\n",
      "epoch 90, step 20, train_loss 44165092.0\n",
      "epoch 90, step 30, train_loss 30901622.0\n",
      "epoch 90, step 40, train_loss 45203280.0\n",
      "epoch 90, step 50, train_loss 29763772.0\n",
      "epoch 90, step 60, train_loss 44766708.0\n",
      "epoch 90, step 70, train_loss 44412400.0\n",
      "epoch 90, step 80, train_loss 16996160.0\n",
      "epoch 90, step 90, train_loss 40693456.0\n",
      "epoch 91, step 10, train_loss 42121964.0\n",
      "epoch 91, step 20, train_loss 27240228.0\n",
      "epoch 91, step 30, train_loss 44015692.0\n",
      "epoch 91, step 40, train_loss 28694424.0\n",
      "epoch 91, step 50, train_loss 29095370.0\n",
      "epoch 91, step 60, train_loss 18506202.0\n",
      "epoch 91, step 70, train_loss 83271960.0\n",
      "epoch 91, step 80, train_loss 78963200.0\n",
      "epoch 91, step 90, train_loss 98488320.0\n",
      "epoch 92, step 10, train_loss 78382896.0\n",
      "epoch 92, step 20, train_loss 52600280.0\n",
      "epoch 92, step 30, train_loss 48507364.0\n",
      "epoch 92, step 40, train_loss 22911608.0\n",
      "epoch 92, step 50, train_loss 52986324.0\n",
      "epoch 92, step 60, train_loss 46007156.0\n",
      "epoch 92, step 70, train_loss 25198208.0\n",
      "epoch 92, step 80, train_loss 50784488.0\n",
      "epoch 92, step 90, train_loss 51990492.0\n",
      "epoch 93, step 10, train_loss 44587976.0\n",
      "epoch 93, step 20, train_loss 23894934.0\n",
      "epoch 93, step 30, train_loss 15612599.0\n",
      "epoch 93, step 40, train_loss 62909996.0\n",
      "epoch 93, step 50, train_loss 69602080.0\n",
      "epoch 93, step 60, train_loss 36798384.0\n",
      "epoch 93, step 70, train_loss 59016544.0\n",
      "epoch 93, step 80, train_loss 42971160.0\n",
      "epoch 93, step 90, train_loss 18987224.0\n",
      "epoch 94, step 10, train_loss 53040524.0\n",
      "epoch 94, step 20, train_loss 21106100.0\n",
      "epoch 94, step 30, train_loss 43486756.0\n",
      "epoch 94, step 40, train_loss 68236480.0\n",
      "epoch 94, step 50, train_loss 39487544.0\n",
      "epoch 94, step 60, train_loss 20974604.0\n",
      "epoch 94, step 70, train_loss 72462848.0\n",
      "epoch 94, step 80, train_loss 20091344.0\n",
      "epoch 94, step 90, train_loss 29619840.0\n",
      "epoch 95, step 10, train_loss 58625132.0\n",
      "epoch 95, step 20, train_loss 40757432.0\n",
      "epoch 95, step 30, train_loss 26812412.0\n",
      "epoch 95, step 40, train_loss 33408382.0\n",
      "epoch 95, step 50, train_loss 34577816.0\n",
      "epoch 95, step 60, train_loss 41995104.0\n",
      "epoch 95, step 70, train_loss 98268304.0\n",
      "epoch 95, step 80, train_loss 60383968.0\n",
      "epoch 95, step 90, train_loss 28447416.0\n",
      "epoch 96, step 10, train_loss 53929896.0\n",
      "epoch 96, step 20, train_loss 50512888.0\n",
      "epoch 96, step 30, train_loss 64074052.0\n",
      "epoch 96, step 40, train_loss 34485252.0\n",
      "epoch 96, step 50, train_loss 94469776.0\n",
      "epoch 96, step 60, train_loss 66846632.0\n",
      "epoch 96, step 70, train_loss 31578290.0\n",
      "epoch 96, step 80, train_loss 51998328.0\n",
      "epoch 96, step 90, train_loss 48536272.0\n",
      "epoch 97, step 10, train_loss 25364284.0\n",
      "epoch 97, step 20, train_loss 64881648.0\n",
      "epoch 97, step 30, train_loss 67343376.0\n",
      "epoch 97, step 40, train_loss 53197972.0\n",
      "epoch 97, step 50, train_loss 25612996.0\n",
      "epoch 97, step 60, train_loss 39921964.0\n",
      "epoch 97, step 70, train_loss 35636560.0\n",
      "epoch 97, step 80, train_loss 56545832.0\n",
      "epoch 97, step 90, train_loss 21048094.0\n",
      "epoch 98, step 10, train_loss 41381488.0\n",
      "epoch 98, step 20, train_loss 43574360.0\n",
      "epoch 98, step 30, train_loss 47778064.0\n",
      "epoch 98, step 40, train_loss 48810328.0\n",
      "epoch 98, step 50, train_loss 33858332.0\n",
      "epoch 98, step 60, train_loss 38130288.0\n",
      "epoch 98, step 70, train_loss 70967528.0\n",
      "epoch 98, step 80, train_loss 43982388.0\n",
      "epoch 98, step 90, train_loss 19980886.0\n",
      "epoch 99, step 10, train_loss 66271224.0\n",
      "epoch 99, step 20, train_loss 50919932.0\n",
      "epoch 99, step 30, train_loss 17236254.0\n",
      "epoch 99, step 40, train_loss 68027552.0\n",
      "epoch 99, step 50, train_loss 70040696.0\n",
      "epoch 99, step 60, train_loss 28572016.0\n",
      "epoch 99, step 70, train_loss 98750992.0\n",
      "epoch 99, step 80, train_loss 82993464.0\n",
      "epoch 99, step 90, train_loss 56714572.0\n",
      "average loss: 42792140.0\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyDNN(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyDNN, self).__init__()\n",
    "\n",
    "        self.linear1 = paddle.nn.Linear(76, 32)\n",
    "        self.linear2 = paddle.nn.Linear(32, 64)\n",
    "        self.linear3 = paddle.nn.Linear(64, 32)\n",
    "        self.linear4 = paddle.nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        x = self.linear1(inputs)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "class MyDataset(paddle.io.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert DataFrame to numpy arrays\n",
    "X = np.array(X).astype('float32')\n",
    "y = np.array(y).astype('float32')\n",
    "\n",
    "# Split your data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "eval_dataset = MyDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = paddle.io.DataLoader(eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyDNN()\n",
    "model.train()\n",
    "\n",
    "# Loss and optimizer\n",
    "mse_loss = paddle.nn.MSELoss()\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters(), \n",
    "                           grad_clip=paddle.nn.ClipGradByNorm(clip_norm=1.0))\n",
    "\n",
    "epochs_num = 100\n",
    "\n",
    "for epochs in range(epochs_num):\n",
    "    for batch_id, data in enumerate(train_loader()):\n",
    "        feature = paddle.to_tensor(data[0])\n",
    "        label = paddle.to_tensor(data[1])\n",
    "        predict = model(feature)\n",
    "        loss = mse_loss(predict, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        if batch_id!=0 and batch_id%10 == 0:\n",
    "            print(f\"epoch {epochs}, step {batch_id}, train_loss {loss.numpy()[0]}\")\n",
    "\n",
    "# Save model\n",
    "paddle.save(model.state_dict(), \"UCIHousingDNN.pdparams\")\n",
    "\n",
    "# Load model for evaluation\n",
    "state_dict = paddle.load(\"UCIHousingDNN.pdparams\")\n",
    "model = MyDNN()\n",
    "model.load_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "losses = []\n",
    "for batch_id, data in enumerate(eval_loader()):\n",
    "    feature = paddle.to_tensor(data[0])\n",
    "    label = paddle.to_tensor(data[1])\n",
    "    predict = model(feature)\n",
    "    loss = mse_loss(predict, label)\n",
    "    losses.append(loss.numpy()[0])\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "print(f\"average loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHFCAYAAAAT5Oa6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdf3RU9Z3/8deQDIFAMhIwmUSCYI0UBa0FDUlFcCE/rDFaVKzRiEiBLhZIgS9KaWXoahDaQtykKlIKaMDUKri21pwEqwib8MPU7AqyWI8sGMgQakMSJE6G5H7/8OSuwwySYJIJuc/HOTnH+7nv+cznvg+nfZ3PnbljMwzDEAAAgIX1CvYCAAAAgo1ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABAAALI9ABKDb2rBhg2w2m8/fpZdeqgkTJujPf/5zp7znsWPH5HK5VFlZ2SnzA+ieCEQAur3169ervLxcZWVlev755xUSEqLbb79df/rTnzr8vY4dO6Zly5YRiACLCQ32AgDgfEaOHKkxY8aYx+np6RowYIBeeukl3X777UFcGYCegh0iABedPn36qHfv3rLb7ebYP//5T82ePVuXXXaZevfurSuuuEJLliyRx+Pxee0f//hHJSYmyuFwKDw8XFdccYUefvhhSdI777yjG264QZI0bdo08zady+UyX//6668rKSlJ4eHhioiIUEpKisrLy33ew+VyyWazaf/+/brvvvvkcDgUExOjhx9+WHV1dZ3UFQDfBIEIQLfX3NysM2fOyOv1qqqqSjk5Ofr888+VlZUlSfriiy90yy236IUXXtD8+fP1xhtv6IEHHtDKlSs1efJkc57y8nLde++9uuKKK1RUVKQ33nhDjz/+uM6cOSNJ+u53v6v169dLkn7+85+rvLxc5eXl+tGPfiRJ2rx5s+644w5FRkbqpZde0rp161RbW6sJEyZo586dfuu+6667dNVVV+nVV1/VY489ps2bN+unP/1pZ7cLwIUwAKCbWr9+vSHJ7y8sLMx45plnzLrnnnvOkGS8/PLLPq9fsWKFIckoKSkxDMMwfv3rXxuSjJMnT57zPffu3WtIMtavX+8z3tzcbMTFxRmjRo0ympubzfGGhgYjOjraSE5ONseWLl1qSDJWrlzpM8fs2bONPn36GC0tLe3uBYDOxQ4RgG7vhRde0N69e7V37169+eabmjp1qh555BEVFBRIkv7617+qX79+uvvuu31e99BDD0mS3nrrLUkyb4dNmTJFL7/8so4ePdrmNRw8eFDHjh1Tdna2evX6v//p7N+/v+666y7t2rVLp0+f9nlNZmamz/G1116rL774QjU1NW1+XwBdg0AEoNsbMWKExowZozFjxig9PV1r1qxRamqqFi1apJMnT+qzzz6T0+mUzWbzeV10dLRCQ0P12WefSZJuvvlmvfbaazpz5owefPBBDR48WCNHjtRLL7103jW0zhEbG+t3Li4uTi0tLaqtrfUZHzhwoM9xWFiYJKmxsbHtFw+gSxCIAFyUrr32WjU2Nuqjjz7SwIEDdfz4cRmG4VNTU1OjM2fOaNCgQebYHXfcobfeekt1dXV65513NHjwYGVlZfl9MPpsreGmurra79yxY8fUq1cvDRgwoAOuDEAwEIgAXJRanxN06aWXauLEiTp16pRee+01n5oXXnhBkjRx4kS/14eFhWn8+PFasWKFJOn99983xyX/XZzhw4frsssu0+bNm32C1+eff65XX33V/OYZgIsTzyEC0O3t27fP/CbYZ599pi1btqi0tFQ/+MEPNGzYMD344IP67W9/q6lTp+p///d/NWrUKO3cuVO5ubn6/ve/r0mTJkmSHn/8cVVVVWnixIkaPHiwTp48qaefflp2u13jx4+XJH3rW99S3759tWnTJo0YMUL9+/dXXFyc4uLitHLlSt1///3KyMjQrFmz5PF49Ktf/UonT57UU089FbT+APjmCEQAur1p06aZ/+1wODRs2DCtWrVKs2fPlvTlc4nefvttLVmyRL/61a904sQJXXbZZVq4cKGWLl1qvjYxMVHvvfeeHn30UZ04cUKXXHKJxowZo7/+9a+65pprJEnh4eH6/e9/r2XLlik1NVVer1dLly6Vy+VSVlaW+vXrp+XLl+vee+9VSEiIxo4dq7ffflvJycld2xQAHcpmnH3THQAAwGL4DBEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8nkPURi0tLTp27JgiIiL8fi8JAAB0T4ZhqKGhQXFxcT4/zHw2AlEbHTt2TPHx8cFeBgAAuACffvqpBg8efM7zQQ1EZ86ckcvl0qZNm+R2uxUbG6uHHnpIP//5z80UZxiGli1bpueff161tbVKTEzUb3/7W/OpspLk8Xi0cOFCvfTSS2psbNTEiRP1zDPP+Fx4bW2t5s6dq9dff12SlJmZqfz8fF1yySVtWmtERISkLxsaGRnZUS2Q1+tVSUmJUlNTZbfbO2zeix198UdPAqMvgdEXf/QksJ7el/r6esXHx5v/P34uQQ1EK1as0HPPPaeNGzfqmmuu0Xvvvadp06bJ4XBo3rx5kqSVK1dq1apV2rBhg6666io98cQTSklJ0cGDB82Ly8nJ0Z/+9CcVFRVp4MCBWrBggTIyMlRRUaGQkBBJUlZWlqqqqlRcXCxJmjlzprKzs/WnP/2pTWttvU0WGRnZ4YEoPDxckZGRPfIf4oWiL/7oSWD0JTD64o+eBGaVvpzv4y5BDUTl5eW64447dNttt0mShg4dqpdeeknvvfeepC93h/Ly8rRkyRJNnjxZkrRx40bFxMRo8+bNmjVrlurq6rRu3Tq9+OKL5g84FhYWKj4+Xtu2bVNaWpoOHDig4uJi7dq1S4mJiZKktWvXKikpSQcPHtTw4cODcPUAAKC7CGoguummm/Tcc8/po48+0lVXXaX/+q//0s6dO5WXlydJOnTokNxut1JTU83XhIWFafz48SorK9OsWbNUUVEhr9frUxMXF6eRI0eqrKxMaWlpKi8vl8PhMMOQJI0dO1YOh0NlZWUBA5HH45HH4zGP6+vrJX2ZpL1eb4f1oHWujpyzJ6Av/uhJYPQlMPrij54E1tP70tbrCmogevTRR1VXV6dvf/vbCgkJUXNzs5588kndd999kiS32y1JiomJ8XldTEyMDh8+bNb07t1bAwYM8Ktpfb3b7VZ0dLTf+0dHR5s1Z1u+fLmWLVvmN15SUqLw8PB2Xun5lZaWdvicPQF98UdPAqMvgdEXf/QksJ7al9OnT7epLqiB6A9/+IMKCwu1efNmXXPNNaqsrFROTo7i4uI0depUs+7s+36GYZz3XuDZNYHqv26exYsXa/78+eZx64eyUlNTO/wzRKWlpUpJSenR927bi774oyeB0ZfA6Is/ehJYT+9L6x2e8wlqIPp//+//6bHHHtMPf/hDSdKoUaN0+PBhLV++XFOnTpXT6ZQk8xtorWpqasxdI6fTqaamJtXW1vrsEtXU1Cg5OdmsOX78uN/7nzhxwm/3qVVYWJjCwsL8xu12e6f8g+mseS929MUfPQmMvgRGX/zRk8B6al/aek1BfVL16dOn/R6SFBISopaWFknSsGHD5HQ6fbbxmpqatH37djPsjB49Wna73aemurpa+/btM2uSkpJUV1enPXv2mDW7d+9WXV2dWQMAAKwrqDtEt99+u5588kkNGTJE11xzjd5//32tWrVKDz/8sKQvb3Pl5OQoNzdXCQkJSkhIUG5ursLDw5WVlSVJcjgcmj59uhYsWKCBAwcqKipKCxcu1KhRo8xvnY0YMULp6emaMWOG1qxZI+nLr91nZGTwDTMAABDcQJSfn69f/OIXmj17tmpqahQXF6dZs2bp8ccfN2sWLVqkxsZGzZ4923wwY0lJic8DllavXq3Q0FBNmTLFfDDjhg0bzGcQSdKmTZs0d+5c89tomZmZKigo6LqLBQAA3VZQA1FERITy8vLMr9kHYrPZ5HK55HK5zlnTp08f5efnKz8//5w1UVFRKiws/CbLBQAAPRS/ZQYA6BLNLc3acWSHqhuqFRsRq3FDximkV8j5Xwh0AQIRAKDTbTmwRfOK56mqvsocGxw5WE+nP63JIyYHcWXAl4L6LTMAQM+35cAW3f3y3T5hSJKO1h/V3S/frS0HtgRpZcD/IRABADpNc0uz5hXPkyHD71zrWE5xjppbmrt6aYAPAhEAoNPsOLLDb2foqwwZ+rT+U+04sqMLVwX4IxABADpNdUN1h9YBnYVABADoNLERsecvakcd0FkIRACATjNuyDgNjhwsmwL/kLZNNsVHxmvckHFdvDLAF4EIANBpQnqF6On0pyXJLxS1Huel5/E8IgQdgQgA0Kkmj5isV6a8ossiL/MZHxw5WK9MeYXnEKFb4MGMAIBON3nEZN0x/A6eVI1ui0AEAOgSIb1CNGHohGAvAwiIW2YAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyCEQAAMDyghqIhg4dKpvN5vf3yCOPSJIMw5DL5VJcXJz69u2rCRMmaP/+/T5zeDwezZkzR4MGDVK/fv2UmZmpqqoqn5ra2lplZ2fL4XDI4XAoOztbJ0+e7LLrBAAA3VtQA9HevXtVXV1t/pWWlkqS7rnnHknSypUrtWrVKhUUFGjv3r1yOp1KSUlRQ0ODOUdOTo62bt2qoqIi7dy5U6dOnVJGRoaam5vNmqysLFVWVqq4uFjFxcWqrKxUdnZ2114sAADotkKD+eaXXnqpz/FTTz2lb33rWxo/frwMw1BeXp6WLFmiyZMnS5I2btyomJgYbd68WbNmzVJdXZ3WrVunF198UZMmTZIkFRYWKj4+Xtu2bVNaWpoOHDig4uJi7dq1S4mJiZKktWvXKikpSQcPHtTw4cO79qIBAEC3E9RA9FVNTU0qLCzU/PnzZbPZ9Mknn8jtdis1NdWsCQsL0/jx41VWVqZZs2apoqJCXq/XpyYuLk4jR45UWVmZ0tLSVF5eLofDYYYhSRo7dqwcDofKysrOGYg8Ho88Ho95XF9fL0nyer3yer0ddt2tc3XknD0BffFHTwKjL4HRF3/0JLCe3pe2Xle3CUSvvfaaTp48qYceekiS5Ha7JUkxMTE+dTExMTp8+LBZ07t3bw0YMMCvpvX1brdb0dHRfu8XHR1t1gSyfPlyLVu2zG+8pKRE4eHhbb+wNmq9XQhf9MUfPQmMvgRGX/zRk8B6al9Onz7dprpuE4jWrVunW2+9VXFxcT7jNpvN59gwDL+xs51dE6j+fPMsXrxY8+fPN4/r6+sVHx+v1NRURUZGfu37t4fX61VpaalSUlJkt9s7bN6LHX3xR08Coy+B0Rd/9CSwnt6X1js859MtAtHhw4e1bds2bdmyxRxzOp2SvtzhiY2NNcdramrMXSOn06mmpibV1tb67BLV1NQoOTnZrDl+/Ljfe544ccJv9+mrwsLCFBYW5jdut9s75R9MZ817saMv/uhJYPQlMPrij54E1lP70tZr6hbPIVq/fr2io6N12223mWPDhg2T0+n02cJramrS9u3bzbAzevRo2e12n5rq6mrt27fPrElKSlJdXZ327Nlj1uzevVt1dXVmDQAAsLag7xC1tLRo/fr1mjp1qkJD/285NptNOTk5ys3NVUJCghISEpSbm6vw8HBlZWVJkhwOh6ZPn64FCxZo4MCBioqK0sKFCzVq1CjzW2cjRoxQenq6ZsyYoTVr1kiSZs6cqYyMDL5hBgAAJHWDQLRt2zYdOXJEDz/8sN+5RYsWqbGxUbNnz1Ztba0SExNVUlKiiIgIs2b16tUKDQ3VlClT1NjYqIkTJ2rDhg0KCQkxazZt2qS5c+ea30bLzMxUQUFB518cAAC4KAQ9EKWmpsowjIDnbDabXC6XXC7XOV/fp08f5efnKz8//5w1UVFRKiws/KZLBQAAPVS3+AwRAABAMBGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5QX9t8wAAEBgzS3N2nFkh6obqhUbEatxQ8YppFfI+V+IdiMQAQDQDW05sEXziuepqr7KHBscOVhPpz+tySMmB3FlPRO3zAAA6Ga2HNiiu1++2ycMSdLR+qO6++W7teXAliCtrOciEAEA0I00tzRrXvE8GTL8zrWO5RTnqLmluauX1qMRiAAA6EZ2HNnhtzP0VYYMfVr/qXYc2dGFq+r5CEQAAHQj1Q3VHVqHtiEQAQDQjcRGxHZoHdqGQAQAQDcybsg4DY4cLJtsAc/bZFN8ZLzGDRnXxSvr2QhEAAB0IyG9QvR0+tOS5BeKWo/z0vN4HlEHIxABANDNTB4xWa9MeUWXRV7mMz44crBemfIKzyHqBDyYEQCAbmjyiMm6Y/gdPKm6ixCIAADopkJ6hWjC0AnBXoYlcMsMAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYXtAD0dGjR/XAAw9o4MCBCg8P13e+8x1VVFSY5w3DkMvlUlxcnPr27asJEyZo//79PnN4PB7NmTNHgwYNUr9+/ZSZmamqqiqfmtraWmVnZ8vhcMjhcCg7O1snT57skmsEAADdW1ADUW1trb73ve/JbrfrzTff1Icffqjf/OY3uuSSS8yalStXatWqVSooKNDevXvldDqVkpKihoYGsyYnJ0dbt25VUVGRdu7cqVOnTikjI0PNzc1mTVZWliorK1VcXKzi4mJVVlYqOzu7S68XAAB0T0H9LbMVK1YoPj5e69evN8eGDh1q/rdhGMrLy9OSJUs0efKXv+y7ceNGxcTEaPPmzZo1a5bq6uq0bt06vfjii5o0aZIkqbCwUPHx8dq2bZvS0tJ04MABFRcXa9euXUpMTJQkrV27VklJSTp48KCGDx/edRcNAAC6naAGotdff11paWm65557tH37dl122WWaPXu2ZsyYIUk6dOiQ3G63UlNTzdeEhYVp/PjxKisr06xZs1RRUSGv1+tTExcXp5EjR6qsrExpaWkqLy+Xw+Eww5AkjR07Vg6HQ2VlZQEDkcfjkcfjMY/r6+slSV6vV16vt8N60DpXR87ZE9AXf/QkMPoSGH3xR08C6+l9aet1BTUQffLJJ3r22Wc1f/58/exnP9OePXs0d+5chYWF6cEHH5Tb7ZYkxcTE+LwuJiZGhw8fliS53W717t1bAwYM8Ktpfb3b7VZ0dLTf+0dHR5s1Z1u+fLmWLVvmN15SUqLw8PD2X+x5lJaWdvicPQF98UdPAqMvgdEXf/QksJ7al9OnT7epLqiBqKWlRWPGjFFubq4k6frrr9f+/fv17LPP6sEHHzTrbDabz+sMw/AbO9vZNYHqv26exYsXa/78+eZxfX294uPjlZqaqsjIyPNfXBt5vV6VlpYqJSVFdru9w+a92NEXf/QkMPoSGH3xR08C6+l9ab3Dcz5BDUSxsbG6+uqrfcZGjBihV199VZLkdDolfbnDExsba9bU1NSYu0ZOp1NNTU2qra312SWqqalRcnKyWXP8+HG/9z9x4oTf7lOrsLAwhYWF+Y3b7fZO+QfTWfNe7OiLP3oSGH0JjL74oyeB9dS+tPWagvots+9973s6ePCgz9hHH32kyy+/XJI0bNgwOZ1On228pqYmbd++3Qw7o0ePlt1u96mprq7Wvn37zJqkpCTV1dVpz549Zs3u3btVV1dn1gAAAOsK6g7RT3/6UyUnJys3N1dTpkzRnj179Pzzz+v555+X9OVtrpycHOXm5iohIUEJCQnKzc1VeHi4srKyJEkOh0PTp0/XggULNHDgQEVFRWnhwoUaNWqU+a2zESNGKD09XTNmzNCaNWskSTNnzlRGRgbfMAMAAMENRDfccIO2bt2qxYsX65e//KWGDRumvLw83X///WbNokWL1NjYqNmzZ6u2tlaJiYkqKSlRRESEWbN69WqFhoZqypQpamxs1MSJE7VhwwaFhISYNZs2bdLcuXPNb6NlZmaqoKCg6y4WAAB0W0ENRJKUkZGhjIyMc5632WxyuVxyuVznrOnTp4/y8/OVn59/zpqoqCgVFhZ+k6UCAIAeKug/3QEAABBsBCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5BCIAAGB5QQ1ELpdLNpvN58/pdJrnDcOQy+VSXFyc+vbtqwkTJmj//v0+c3g8Hs2ZM0eDBg1Sv379lJmZqaqqKp+a2tpaZWdny+FwyOFwKDs7WydPnuySawQAAN1f0HeIrrnmGlVXV5t/H3zwgXlu5cqVWrVqlQoKCrR37145nU6lpKSooaHBrMnJydHWrVtVVFSknTt36tSpU8rIyFBzc7NZk5WVpcrKShUXF6u4uFiVlZXKzs7u0usEAADdV2jQFxAa6rMr1MowDOXl5WnJkiWaPHmyJGnjxo2KiYnR5s2bNWvWLNXV1WndunV68cUXNWnSJElSYWGh4uPjtW3bNqWlpenAgQMqLi7Wrl27lJiYKElau3atkpKSdPDgQQ0fPrzrLhYAAHRLQd8h+vvf/664uDgNGzZMP/zhD/XJJ59Ikg4dOiS3263U1FSzNiwsTOPHj1dZWZkkqaKiQl6v16cmLi5OI0eONGvKy8vlcDjMMCRJY8eOlcPhMGsAAIC1BXWHKDExUS+88IKuuuoqHT9+XE888YSSk5O1f/9+ud1uSVJMTIzPa2JiYnT48GFJktvtVu/evTVgwAC/mtbXu91uRUdH+713dHS0WROIx+ORx+Mxj+vr6yVJXq9XXq/3Aq42sNa5OnLOnoC++KMngdGXwOiLP3oSWE/vS1uvK6iB6NZbbzX/e9SoUUpKStK3vvUtbdy4UWPHjpUk2Ww2n9cYhuE3drazawLVn2+e5cuXa9myZX7jJSUlCg8P/9r3vxClpaUdPmdPQF/80ZPA6Etg9MUfPQmsp/bl9OnTbaoL+meIvqpfv34aNWqU/v73v+vOO++U9OUOT2xsrFlTU1Nj7ho5nU41NTWptrbWZ5eopqZGycnJZs3x48f93uvEiRN+u09ftXjxYs2fP988rq+vV3x8vFJTUxUZGfnNLvQrvF6vSktLlZKSIrvd3mHzXuzoiz96Ehh9CYy++KMngfX0vrTe4TmfbhWIPB6PDhw4oHHjxmnYsGFyOp0qLS3V9ddfL0lqamrS9u3btWLFCknS6NGjZbfbVVpaqilTpkiSqqurtW/fPq1cuVKSlJSUpLq6Ou3Zs0c33nijJGn37t2qq6szQ1MgYWFhCgsL8xu32+2d8g+ms+a92NEXf/QkMPoSGH3xR08C66l9aes1BTUQLVy4ULfffruGDBmimpoaPfHEE6qvr9fUqVNls9mUk5Oj3NxcJSQkKCEhQbm5uQoPD1dWVpYkyeFwaPr06VqwYIEGDhyoqKgoLVy4UKNGjTK/dTZixAilp6drxowZWrNmjSRp5syZysjI4BtmAABAUpADUVVVle677z794x//0KWXXqqxY8dq165duvzyyyVJixYtUmNjo2bPnq3a2lolJiaqpKREERER5hyrV69WaGiopkyZosbGRk2cOFEbNmxQSEiIWbNp0ybNnTvX/DZaZmamCgoKuvZiAQBAtxXUQFRUVPS15202m1wul1wu1zlr+vTpo/z8fOXn55+zJioqSoWFhRe6TAAA0MMF/TlEAAAAwUYgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAltfuQPTCCy/4/Ohpq6amJr3wwgsdsigAAICu1O5ANG3aNNXV1fmNNzQ0aNq0aR2yKAAAgK7U7kB0rl+Jr6qqksPh6JBFAQAAdKU2P6n6+uuvl81mk81m08SJExUa+n8vbW5u1qFDh5Sent4piwQAAD1Tc0uzdhzZoeqGasVGxGrckHEK6RVy/hd2sDYHojvvvFOSVFlZqbS0NPXv398817t3bw0dOlR33XVXx68QAAD0SFsObNG84nmqqq8yxwZHDtbT6U9r8ojJXbqWNgeipUuXSpKGDh2qe++9V3369Om0RQEAgJ5ty4Etuvvlu2XI8Bk/Wn9Ud798t16Z8kqXhqJ2f4Zo6tSphCEAAHDBmluaNa94nl8YkmSO5RTnqLmlucvW1KYdogEDBgT8IHUg//znP7/RggAAQM+248gOn9tkZzNk6NP6T7XjyA5NGDqhS9bUpkCUl5fX2esAAAAWUd1Q3aF1HaFNgWjq1KmdvQ4AAGARsRGxHVrXEdr8oepWR44c+drzQ4YMueDFAACAnm/ckHEaHDlYR+uPBvwckU02DY4crHFDxnXZmtodiIYOHfq1nydqbu66D0ABAICLT0ivED2d/rTufvlu2WTzCUU2fZkx8tLzuvR5RO0ORO+//77Psdfr1fvvv69Vq1bpySef7LCFAQCAnmvyiMl6ZcorAZ9DlJee132fQ9Tquuuu8xsbM2aM4uLi9Ktf/UqTJ3ftBQAAgIvT5BGTdcfwOy6uJ1Wfz1VXXaW9e/d21HQAAMACQnqFdNlX679OuwNRfX29z7FhGKqurpbL5VJCQkKHLQwAAKCrtDsQXXLJJX4fqjYMQ/Hx8SoqKuqwhQEAAHSVdgeit99+2+e4V69euvTSS3XllVcqNLTD7sABAAB0mXYnmPHjx3fGOgAAAIKm3T/uunHjRr3xxhvm8aJFi3TJJZcoOTlZhw8f7tDFAQAAdIV2B6Lc3Fz17dtXklReXq6CggKtXLlSgwYN0k9/+uBgfm4AACAASURBVNMOXyAAAEBna/cts08//VRXXnmlJOm1117T3XffrZkzZ+p73/ueJkyY0NHrAwAA6HTt3iHq37+/PvvsM0lSSUmJJk2aJEnq06ePGhsbO3Z1AAAAXaDdO0QpKSn60Y9+pOuvv14fffSRbrvtNknS/v37NXTo0I5eHwAAQKdr9w7Rb3/7WyUlJenEiRN69dVXNXDgQElSRUWF7rvvvg5fIAAAQGe7oAczFhQU+I0vW7asQxYEAADQ1dq9QyRJO3bs0AMPPKDk5GQdPXpUkvTiiy9q586dHbo4AACArtDuQPTqq68qLS1Nffv21d/+9jd5PB5JUkNDg3Jzcy94IcuXL5fNZlNOTo45ZhiGXC6X4uLi1LdvX02YMEH79+/3eZ3H49GcOXM0aNAg9evXT5mZmaqqqvKpqa2tVXZ2thwOhxwOh7Kzs3Xy5MkLXisAAOhZ2h2InnjiCT333HNau3at7Ha7OZ6cnKy//e1vF7SIvXv36vnnn9e1117rM75y5UqtWrVKBQUF2rt3r5xOp1JSUtTQ0GDW5OTkaOvWrSoqKtLOnTt16tQpZWRkqLm52azJyspSZWWliouLVVxcrMrKSmVnZ1/QWgEAQM/T7kB08OBB3XzzzX7jkZGRF7TrcurUKd1///1au3atBgwYYI4bhqG8vDwtWbJEkydP1siRI7Vx40adPn1amzdvliTV1dVp3bp1+s1vfqNJkybp+uuvV2FhoT744ANt27ZNknTgwAEVFxfrd7/7nZKSkpSUlKS1a9fqz3/+sw4ePNju9QIAgJ6n3R+qjo2N1ccff+z3FfudO3fqiiuuaPcCHnnkEd12222aNGmSnnjiCXP80KFDcrvdSk1NNcfCwsI0fvx4lZWVadasWaqoqJDX6/WpiYuL08iRI1VWVqa0tDSVl5fL4XAoMTHRrBk7dqwcDofKyso0fPjwgOvyeDzm7UBJqq+vlyR5vV55vd52X+e5tM7VkXP2BPTFHz0JjL4ERl/80ZPAenpf2npd7Q5Es2bN0rx58/T73/9eNptNx44dU3l5uRYuXKjHH3+8XXMVFRWpoqJC7733nt85t9stSYqJifEZj4mJMX8zze12q3fv3j47S601ra93u92Kjo72mz86OtqsCWT58uUBvzlXUlKi8PDw81xZ+5WWlnb4nD0BffFHTwKjL4HRF3/0JLCe2pfTp0+3qa7dgWjRokWqq6vTLbfcoi+++EI333yzwsLCtHDhQv3kJz9p8zyffvqp5s2bp5KSEvXp0+ecdTabzefYMAy/sbOdXROo/nzzLF68WPPnzzeP6+vrFR8fr9TUVEVGRn7t+7eH1+tVaWmpUlJSfD6TZXX0xR89CYy+BEZf/NGTwHp6X1rv8JxPuwORJD355JNasmSJPvzwQ7W0tOjqq69W//792zVHRUWFampqNHr0aHOsublZ7777rgoKCszP97jdbsXGxpo1NTU15q6R0+lUU1OTamtrfXaJampqlJycbNYcP37c7/1PnDjht/v0VWFhYQoLC/Mbt9vtnfIPprPmvdjRF3/0JDD6Ehh98UdPAuupfWnrNbXrQ9VnzpxRaGio9u3bp/DwcI0ZM0Y33nhju8OQJE2cOFEffPCBKisrzb8xY8bo/vvvV2Vlpa644go5nU6fLbympiZt377dDDujR4+W3W73qamurta+ffvMmqSkJNXV1WnPnj1mze7du1VXV2fWAAAAa2vXDlFoaKguv/xyn6+0X6iIiAiNHDnSZ6xfv34aOHCgOZ6Tk6Pc3FwlJCQoISFBubm5Cg8PV1ZWliTJ4XBo+vTpWrBggQYOHKioqCgtXLhQo0aNMn90dsSIEUpPT9eMGTO0Zs0aSdLMmTOVkZFxzg9UAwAAa2n3LbOf//znWrx4sQoLCxUVFdUZazItWrRIjY2Nmj17tmpra5WYmKiSkhJFRESYNatXr1ZoaKimTJmixsZGTZw4URs2bFBISIhZs2nTJs2dO9f8NlpmZmbAnx8BAADW1O5A9O///u/6+OOPFRcXp8svv1z9+vXzOX+hD2eUpHfeecfn2GazyeVyyeVynfM1ffr0UX5+vvLz889ZExUVpcLCwgteFwAA6NnaHYjuvPPOzlgHAABA0LQ7EC1durQz1gEAABA0F/Rr9wAAAD0JgQgAAFgegQgAAFgegQgAAFheuwKR1+vVFVdcoQ8//LCz1gMAANDl2hWI7Ha7PB7PeX9cFQAA4GLS7ltmc+bM0YoVK3TmzJnOWA8AAECXa/dziHbv3q233npLJSUlGjVqlN+Tqrds2dJhiwMAAOgK7Q5El1xyie66667OWAsAAEBQtDsQrV+/vjPWAQAAEDQX9LX7M2fOaNu2bVqzZo0aGhokSceOHdOpU6c6dHEAAABdod07RIcPH1Z6erqOHDkij8ejlJQURUREaOXKlfriiy/03HPPdcY6AQAAOk27d4jmzZunMWPGqLa2Vn379jXHf/CDH+itt97q0MUBAAB0hXbvEO3cuVP/+Z//qd69e/uMX3755Tp69GiHLQwAAKCrtHuHqKWlRc3NzX7jVVVVioiI6JBFAQAAdKV2B6KUlBTl5eWZxzabTadOndLSpUv1/e9/v0MXBwAA0BXafcts9erVuuWWW3T11Vfriy++UFZWlv7+979r0KBBeumllzpjjQAAAJ2q3YEoLi5OlZWVKioqUkVFhVpaWjR9+nTdf//9Ph+yBgAAuFi0KRB997vf1VtvvaUBAwbol7/8pRYuXKhp06Zp2rRpnb0+AACATtemzxAdOHBAn3/+uSRp2bJlPIARAAD0KG3aIfrOd76jadOm6aabbpJhGPr1r3+t/v37B6x9/PHHO3SBAAAAna1NgWjDhg1aunSp/vznP8tms+nNN99UaKj/S202G4EIAABcdNoUiIYPH66ioiJJUq9evfTWW28pOjq6UxcGAADQVdr9LbOWlpbOWAcAAEDQtDsQSdJHH32kd955RzU1NX4BiVtmAADgYtPuQLR27Vr967/+qwYNGiSn0ymbzWae4zNEAADgYtTuQPTEE0/oySef1KOPPtoZ6wEAAOhy7f4ts9raWt1zzz2dsRYAAICgaHcguueee1RSUtIZawEAAAiKdt8yu/LKK/WLX/xCu3bt0qhRo2S3233Oz507t8MWBwAA0BXavUP0/PPPq3///tq+fbsKCgq0evVq8y8vL69dcz377LO69tprFRkZqcjISCUlJenNN980zxuGIZfLpbi4OPXt21cTJkzQ/v37febweDyaM2eOBg0apH79+ikzM1NVVVU+NbW1tcrOzpbD4ZDD4VB2drZOnjzZ3ksHAAA9VLsD0aFDh87598knn7RrrsGDB+upp57Se++9p/fee0//8i//ojvuuMMMPStXrtSqVatUUFCgvXv3yul0KiUlRQ0NDeYcOTk52rp1q4qKirRz506dOnVKGRkZam5uNmuysrJUWVmp4uJiFRcXq7KyUtnZ2e29dAAA0ENd0HOIOsrtt9/uc/zkk0/q2Wef1a5du3T11VcrLy9PS5Ys0eTJkyVJGzduVExMjDZv3qxZs2aprq5O69at04svvqhJkyZJkgoLCxUfH69t27YpLS1NBw4cUHFxsXbt2qXExERJXz46ICkpSQcPHtTw4cO79qIBAEC306ZANH/+fP3bv/2b+vXrp/nz539t7apVqy5oIc3NzfrjH/+ozz//XElJSTp06JDcbrdSU1PNmrCwMI0fP15lZWWaNWuWKioq5PV6fWri4uI0cuRIlZWVKS0tTeXl5XI4HGYYkqSxY8fK4XCorKyMQAQAANoWiN5//315vV7zv8/lqw9pbKsPPvhASUlJ+uKLL9S/f39t3bpVV199tcrKyiRJMTExPvUxMTE6fPiwJMntdqt3794aMGCAX43b7TZrAv3uWnR0tFkTiMfjkcfjMY/r6+slSV6v1+xFR2idqyPn7Anoiz96Ehh9CYy++KMngfX0vrT1utoUiN5+++2A/90Rhg8frsrKSp08eVKvvvqqpk6dqu3bt5vnzw5ZhmGcN3idXROo/nzzLF++XMuWLfMbLykpUXh4+Ne+/4UoLS3t8Dl7Avrij54ERl8Coy/+6ElgPbUvp0+fblNdUD9DJEm9e/fWlVdeKUkaM2aM9u7dq6efftp8Erbb7VZsbKxZX1NTY+4aOZ1ONTU1qba21meXqKamRsnJyWbN8ePH/d73xIkTfrtPX7V48WKf24P19fWKj49XamqqIiMjv8EV+/J6vSotLVVKSorfIwysjL74oyeB0ZfA6Is/ehJYT+9L6x2e8wl6IDqbYRjyeDwaNmyYnE6nSktLdf3110uSmpqatH37dq1YsUKSNHr0aNntdpWWlmrKlCmSpOrqau3bt08rV66UJCUlJamurk579uzRjTfeKEnavXu36urqzNAUSFhYmMLCwvzG7XZ7p/yD6ax5L3b0xR89CYy+BEZf/NGTwHpqX9p6TUENRD/72c906623Kj4+Xg0NDSoqKtI777yj4uJi2Ww25eTkKDc3VwkJCUpISFBubq7Cw8OVlZUlSXI4HJo+fboWLFiggQMHKioqSgsXLtSoUaPMb52NGDFC6enpmjFjhtasWSNJmjlzpjIyMvhANQAAkBTkQHT8+HFlZ2erurpaDodD1157rYqLi5WSkiJJWrRokRobGzV79mzV1tYqMTFRJSUlioiIMOdYvXq1QkNDNWXKFDU2NmrixInasGGDQkJCzJpNmzZp7ty55rfRMjMzVVBQ0LUXCwAAuq2gBqJ169Z97XmbzSaXyyWXy3XOmj59+ig/P1/5+fnnrImKilJhYeGFLhMAAPRw7X5SNQAAQE9DIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJZHIAIAAJYX1EC0fPly3XDDDYqIiFB0dLTuvPNOHTx40KfGMAy5XC7FxcWpb9++mjBhgvbv3+9T4/F4NGfOHA0aNEj9+vVTZmamqqqqfGpqa2uVnZ0th8Mhh8Oh7OxsnTx5stOvEQAAdH9BDUTbt2/XI488ol27dqm0tFRnzpxRamqqPv/8c7Nm5cqVWrVqlQoKCrR37145nU6lpKSooaHBrMnJydHWrVtVVFSknTt36tSpU8rIyFBzc7NZk5WVpcrKShUXF6u4uFiVlZXKzs7u0usFAADdU2gw37y4uNjneP369YqOjlZFRYVuvvlmGYahvLw8LVmyRJMnT5Ykbdy4UTExMdq8ebNmzZqluro6rVu3Ti+++KImTZokSSosLFR8fLy2bdumtLQ0HThwQMXFxdq1a5cSExMlSWvXrlVSUpIOHjyo4cOHd+2FAwCAbiWogehsdXV1kqSoqChJ0qFDh+R2u5WammrWhIWFafz48SorK9OsWbNUUVEhr9frUxMXF6eRI0eqrKxMaWlpKi8vl8PhMMOQJI0dO1YOh0NlZWUBA5HH45HH4zGP6+vrJUler1der7fDrrl1ro6csyegL/7oSWD0JTD64o+eBNbT+9LW6+o2gcgwDM2fP1833XSTRo4cKUlyu92SpJiYGJ/amJgYHT582Kzp3bu3BgwY4FfT+nq3263o6Gi/94yOjjZrzrZ8+XItW7bMb7ykpETh4eHtvLrzKy0t7fA5ewL64o+eBEZfAqMv/uhJYD21L6dPn25TXbcJRD/5yU/03//939q5c6ffOZvN5nNsGIbf2NnOrglU/3XzLF68WPPnzzeP6+vrFR8fr9TUVEVGRn7te7eH1+tVaWmpUlJSZLfbO2zeix198UdPAqMvgdEXf/QksJ7el9Y7POfTLQLRnDlz9Prrr+vdd9/V4MGDzXGn0ynpyx2e2NhYc7ympsbcNXI6nWpqalJtba3PLlFNTY2Sk5PNmuPHj/u974kTJ/x2n1qFhYUpLCzMb9xut3fKP5jOmvdiR1/80ZPA6Etg9MUfPQmsp/alrdcU1G+ZGYahn/zkJ9qyZYv++te/atiwYT7nhw0bJqfT6bON19TUpO3bt5thZ/To0bLb7T411dXV2rdvn1mTlJSkuro67dmzx6zZvXu36urqzBoAAGBdQd0heuSRR7R582b9x3/8hyIiIszP8zgcDvXt21c2m005OTnKzc1VQkKCEhISlJubq/DwcGVlZZm106dP14IFCzRw4EBFRUVp4cKFGjVqlPmtsxEjRig9PV0zZszQmjVrJEkzZ85URkYG3zADAADBDUTPPvusJGnChAk+4+vXr9dDDz0kSVq0aJEaGxs1e/Zs1dbWKjExUSUlJYqIiDDrV69erdDQUE2ZMkWNjY2aOHGiNmzYoJCQELNm06ZNmjt3rvlttMzMTBUUFHTuBQIAgItCUAORYRjnrbHZbHK5XHK5XOes6dOnj/Lz85Wfn3/OmqioKBUWFl7IMgEAQA/Hb5kBAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLIxABAADLC2ogevfdd3X77bcrLi5ONptNr732ms95wzDkcrkUFxenvn37asKECdq/f79Pjcfj0Zw5czRo0CD169dPmZmZqqqq8qmpra1Vdna2HA6HHA6HsrOzdfLkyU6/PgAAcHEIaiD6/PPPdd1116mgoCDg+ZUrV2rVqlUqKCjQ3r175XQ6lZKSooaGBrMmJydHW7duVVFRkXbu3KlTp04pIyNDzc3NZk1WVpYqKytVXFys4uJiVVZWKjs7u9OvDwAAXBxCg/nmt956q2699daA5wzDUF5enpYsWaLJkydLkjZu3KiYmBht3rxZs2bNUl1dndatW6cXX3xRkyZNkiQVFhYqPj5e27ZtU1pamg4cOKDi4mLt2rVLiYmJkqS1a9cqKSlJBw8e1PDhw7vmYgEAQLfVbT9DdOjQIbndbqWmpppjYWFhGj9+vMrKyiRJFRUV8nq9PjVxcXEaOXKkWVNeXi6Hw2GGIUkaO3asHA6HWQMAAKwtqDtEX8ftdkuSYmJifMZjYmJ0+PBhs6Z3794aMGCAX03r691ut6Kjo/3mj46ONmsC8Xg88ng85nF9fb0kyev1yuv1XsAVBdY6V0fO2RPQF3/0JDD6Ehh98UdPAuvpfWnrdXXbQNTKZrP5HBuG4Td2trNrAtWfb57ly5dr2bJlfuMlJSUKDw8/37LbrbS0tMPn7Anoiz96Ehh9CYy++KMngfXUvpw+fbpNdd02EDmdTklf7vDExsaa4zU1NeaukdPpVFNTk2pra312iWpqapScnGzWHD9+3G/+EydO+O0+fdXixYs1f/5887i+vl7x8fFKTU1VZGTkN7u4r/B6vSotLVVKSorsdnuHzXuxoy/+6Elg9CUw+uKPngTW0/vSeofnfLptIBo2bJicTqdKS0t1/fXXS5Kampq0fft2rVixQpI0evRo2e12lZaWasqUKZKk6upq7du3TytXrpQkJSUlqa6uTnv27NGNN94oSdq9e7fq6urM0BRIWFiYwsLC/Mbtdnun/IPprHkvdvTFHz0JjL4ERl/80ZPAempf2npNQQ1Ep06d0scff2weHzp0SJWVlYqKitKQIUOUk5Oj3NxcJSQkKCEhQbm5uQoPD1dWVpYkyeFwaPr06VqwYIEGDhyoqKgoLVy4UKNGjTK/dTZixAilp6drxowZWrNmjSRp5syZysjI4BtmAABAUpAD0XvvvadbbrnFPG69RTV16lRt2LBBixYtUmNjo2bPnq3a2lolJiaqpKREERER5mtWr16t0NBQTZkyRY2NjZo4caI2bNigkJAQs2bTpk2aO3eu+W20zMzMcz77CAAAWE9QA9GECRNkGMY5z9tsNrlcLrlcrnPW9OnTR/n5+crPzz9nTVRUlAoLC7/JUgEAQA/WbZ9DBAAA0FUIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIsFYieeeYZDRs2TH369NHo0aO1Y8eOYC8JAAB0A5YJRH/4wx+Uk5OjJUuW6P3339e4ceN066236siRI8FemiTpRINHLS1GsJcBAIAlWSYQrVq1StOnT9ePfvQjjRgxQnl5eYqPj9ezzz4b1HU1txja4bYp5emdemlv9whnAABYTWiwF9AVmpqaVFFRoccee8xnPDU1VWVlZQFf4/F45PF4zOP6+npJktfrldfr7ZB1nWlu0X2/26PKqhBJzSr+oFr3XB8rm83WIfNfzFp73FG97gnoSWD0JTD64o+eBNbT+9LW67JEIPrHP/6h5uZmxcTE+IzHxMTI7XYHfM3y5cu1bNkyv/GSkhKFh4d32NoGNPdSnxCbMoa06HuDjuvNN9/ssLl7gtLS0mAvoduhJ4HRl8Doiz96ElhP7cvp06fbVGeJQNTq7J0XwzDOuRuzePFizZ8/3zyur69XfHy8UlNTFRkZ2WFrSv68UW+Wvq27b0uR3W7vsHkvdl6vV6WlpUpJoS+t6Elg9CUw+uKPngTW0/vSeofnfCwRiAYNGqSQkBC/3aCamhq/XaNWYWFhCgsL8xu32+0d+g/mkn6So3fHz9tT0Bd/9CQw+hIYffFHTwLrqX1p6zVZ4kPVvXv31ujRo/22A0tLS5WcnBykVQEAgO7CEjtEkjR//nxlZ2drzJgxSkpK0vPPP68jR47oxz/+cbCXBgAAgswygejee+/VZ599pl/+8peqrq7WyJEj9Ze//EWXX355sJcGAACCzDKBSJJmz56t2bNnB3sZAACgm7HEZ4gAAAC+DoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYnqWeVP1NGIYhSaqvr+/Qeb1er06fPq36+voe+SvDF4q++KMngdGXwOiLP3oSWE/vS+v/b7f+//i5EIjaqKGhQZIUHx8f5JUAAID2amhokMPhOOd5m3G+yARJUktLi44dO6aIiAjZbLYOm7e+vl7x8fH69NNPFRkZ2WHzXuzoiz96Ehh9CYy++KMngfX0vhiGoYaGBsXFxalXr3N/Uogdojbq1auXBg8e3GnzR0ZG9sh/iN8UffFHTwKjL4HRF3/0JLCe3Jev2xlqxYeqAQCA5RGIAACA5YW4XC5XsBdhdSEhIZowYYJCQ7mD+VX0xR89CYy+BEZf/NGTwOgLH6oGAADglhkAAACBCAAAWB6BCAAAWB6BCAAAWB6BKMieeeYZDRs2TH369NHo0aO1Y8eOYC+pw7z77ru6/fbbFRcXJ5vNptdee83nvGEYcrlciouLU9++fTVhwgTt37/fp8bj8WjOnDkaNGiQ+vXrp8zMTFVVVfnU1NbWKjs7Ww6HQw6HQ9nZ2Tp58mSnX9+FWL58uW644QZFREQoOjpad955pw4ePOhTY7W+PPvss7r22mvNh8IlJSXpzTffNM9brR/nsnz5ctlsNuXk5JhjVuuNy+WSzWbz+XM6neZ5q/Xjq44ePaoHHnhAAwcOVHh4uL7zne+ooqLCPG/l3rSZgaApKioy7Ha7sXbtWuPDDz805s2bZ/Tr1884fPhwsJfWIf7yl78YS5YsMV599VVDkrF161af80899ZQRERFhvPrqq8YHH3xg3HvvvUZsbKxRX19v1vz4xz82LrvsMqO0tNT429/+Ztxyyy3GddddZ5w5c8asSU9PN0aOHGmUlZUZZWVlxsiRI42MjIwuu872SEtLM9avX2/s27fPqKysNG677TZjyJAhxqlTp8waq/Xl9ddfN9544w3j4MGDxsGDB42f/exnht1uN/bt22cYhvX6EciePXuMoUOHGtdee60xb948c9xqvVm6dKlxzTXXGNXV1eZfTU2Ned5q/Wj1z3/+07j88suNhx56yNi9e7dx6NAhY9u2bcbHH39s1li1N+1BIAqiG2+80fjxj3/sM/btb3/beOyxx4K0os5zdiBqaWkxnE6n8dRTT5ljX3zxheFwOIznnnvOMAzDOHnypGG3242ioiKz5ujRo0avXr2M4uJiwzAM48MPPzQkGbt27TJrysvLDUnG//zP/3T2ZX1jNTU1hiRj+/bthmHQl1YDBgwwfve739EPwzAaGhqMhIQEo7S01Bg/frwZiKzYm6VLlxrXXXddwHNW7EerRx991LjpppvOed7KvWkPbpkFSVNTkyoqKpSamuoznpqaqrKysiCtquscOnRIbrfb5/rDwsI0fvx48/orKirk9Xp9auL+fzv3HxN1/ccB/HnnccDB7ZQEDmIJYiEXyJBLPWll4B/+wNlabdUtz+FqQmf5q7V0rcZKcqFWs3RS/sBsbk7cMNsQFz/MYTS8ywPE2AipBuFPImDCjvf3j3afr+chcCYe+X4+ttvu3p/X+32vz2vH8drd532xsUhJSVFiamtrYTAYMHfuXCVmmU+UUwAACeBJREFU3rx5MBgM/4k6dnd3AwAiIiIAsC5utxuHDx9Gb28vLBaL9PUAgNdffx1Lly7FwoULvcZlrU1LSwtiY2ORkJCAF198Ea2trQDkrQcAlJWVwWw244UXXkBUVBTS09NRXFysHJe5Nv5gQxQgV65cgdvtRnR0tNd4dHQ0Ojs7A5TV/eM5x5HOv7OzE1qtFlOmTBkxJioqymf9qKioCV9HIQTWr1+PJ598EikpKQDkrYvL5UJ4eDiCg4OxevVqHDt2DCaTSdp6eBw+fBj19fUoLCz0OSZjbebOnYuSkhKUl5ejuLgYnZ2dmD9/Pq5evSplPTxaW1uxa9cuPProoygvL8fq1avxxhtvoKSkBICcr5W7Ie9vdE8QKpXK67EQwmfsQXY35397zHDx/4U62u12nD9/Hj/88IPPMdnqkpSUBKfTiRs3buDo0aOw2Wyorq5WjstWDwD47bff8Oabb+LkyZMICQm5Y5xMtVm8eLFyPzU1FRaLBYmJiThw4ADmzZsHQK56eAwNDcFsNmPLli0AgPT0dDQ2NmLXrl1YsWKFEidjbfzBT4gCZOrUqZg0aZJPV93V1eXTxT+IPDtDRjp/o9GIgYEBXL9+fcSYP//802f9y5cvT+g6rlmzBmVlZaisrERcXJwyLmtdtFotZsyYAbPZjMLCQqSlpeHTTz+Vth7AP19hdHV1ISMjAxqNBhqNBtXV1fjss8+g0WiUvGWsjUdYWBhSU1PR0tIi9WslJiYGJpPJayw5ORnt7e0A5H1f8RcbogDRarXIyMhARUWF13hFRQXmz58foKzun4SEBBiNRq/zHxgYQHV1tXL+GRkZCAoK8orp6OhAQ0ODEmOxWNDd3Y26ujol5scff0R3d/eErKMQAna7HaWlpfj++++RkJDgdVzWutxOCIGbN29KXY/s7Gy4XC44nU7lZjabYbVa4XQ6MX36dGlr43Hz5k1cuHABMTExUr9WMjMzfX6+45dffsG0adMA8H1lzO7nFdzkzbPt/quvvhJNTU1i7dq1IiwsTLS1tQU6tXuip6dHOBwO4XA4BACxfft24XA4lJ8V+Oijj4TBYBClpaXC5XKJl156adhtoHFxceLUqVPi3LlzIisra9htoLNmzRK1tbWitrZWpKamTthtoHl5ecJgMIiqqiqvrcN9fX1KjGx1eeedd0RNTY349ddfxfnz58WmTZuEWq0WJ0+eFELIV4+R3LrLTAj5arNhwwZRVVUlWltbxdmzZ0VOTo7Q6/XKe6Zs9fCoq6sTGo1GfPjhh6KlpUUcOnRI6HQ68fXXXysxstbGH2yIAuzzzz8X06ZNE1qtVsyePVvZfv0gqKysFAB8bjabTQjxz1bQ9957TxiNRhEcHCyeeuop4XK5vNbo7+8XdrtdREREiNDQUJGTkyPa29u9Yq5evSqsVqvQ6/VCr9cLq9Uqrl+/fr9O0y/D1QOA2LdvnxIjW11yc3OVv4HIyEiRnZ2tNENCyFePkdzeEMlWG89v5wQFBYnY2Fjx3HPPicbGRuW4bPW41fHjx0VKSooIDg4WM2fOFHv27PE6LnNtxkolhBCB+WyKiIiIaGLgNUREREQkPTZEREREJD02RERERCQ9NkREREQkPTZEREREJD02RERERCQ9NkREREQkPTZERES3aGtrg0qlgtPpDGge+/fvx+TJkwOaA5FM2BAREfkpPj4en3zyyYRdj4j8x4aIiCaUgYGBQKdwT7jdbgwNDQU6DSIaIzZERDRuenp6YLVaERYWhpiYGOzYsQMLFizA2rVrlZj4+Hh88MEHWLlyJQwGA1599VUAgMvlQlZWFkJDQ/HQQw/htddew99//63Mu30dAHj22WexcuVKr7W3bNmC3Nxc6PV6PPLII9izZ4/XnLq6OqSnpyMkJARmsxkOh2PEc1qwYAEuXbqEdevWQaVSQaVSAfj/V1zffvstTCYTgoODcenSpVHzvNN6HuXl5UhOTkZ4eDgWLVqEjo6OEfMjorvDhoiIxs369etx5swZlJWVoaKiAqdPn8a5c+d84j7++GOkpKSgvr4e7777Lvr6+rBo0SJMmTIFP/30E44cOYJTp07Bbrf7ncO2bduURic/Px95eXlobm4GAPT29iInJwdJSUmor6/H+++/j40bN464XmlpKeLi4lBQUICOjg6vBqWvrw+FhYX48ssv0djYiKioqFHzG229oqIiHDx4EDU1NWhvbx81PyK6O5pAJ0BED6aenh4cOHAA33zzDbKzswEA+/btQ2xsrE9sVlaW1z/64uJi9Pf3o6SkBGFhYQCAnTt3YtmyZdi6dSuio6PHnMeSJUuQn58PAHj77bexY8cOVFVVYebMmTh06BDcbjf27t0LnU6Hxx9/HL///jvy8vLuuF5ERAQmTZoEvV4Po9HodWxwcBBffPEF0tLSxpzfaOvt3r0biYmJAAC73Y6CgoIxr01EY8dPiIhoXLS2tmJwcBBz5sxRxgwGA5KSknxizWaz1+MLFy4gLS1NaYYAIDMzE0NDQ7h48aJfecyaNUu5r1KpYDQa0dXV5fU8Op1OibFYLH6tfyutVuv1fP+WTqdTmiEAiImJUXInonuLDRERjQshBAD4XBPjGb/VrY2PJ+b2eR6ecbVa7bPW4OCgT3xQUJDPfM/FzsPl8m+Ehob65D3WPIczXO73Omci+gcbIiIaF4mJiQgKCkJdXZ0y9tdff6GlpWXUuSaTCU6nE729vcrYmTNnoFar8dhjjwEAIiMjva63cbvdaGho8CtHk8mEn3/+Gf39/crY2bNnR52n1WrhdrvH9BxjydOf9YhofLAhIqJxodfrYbPZ8NZbb6GyshKNjY3Izc2FWq2+46c/HlarFSEhIbDZbGhoaEBlZSXWrFmDV155Rbl+KCsrCydOnMCJEyfQ3NyM/Px83Lhxw68cX375ZajVaqxatQpNTU347rvvUFRUNOq8+Ph41NTU4I8//sCVK1dGjB1Lnv6sR0Tjgw0REY2b7du3w2KxICcnBwsXLkRmZiaSk5MREhIy4jydTofy8nJcu3YNTzzxBJ5//nlkZ2dj586dSkxubi5sNhtWrFiBp59+GgkJCXjmmWf8yi88PBzHjx9HU1MT0tPTsXnzZmzdunXUeQUFBWhra0NiYiIiIyNHjB1Lnv6sR0TjQyX4hTQR3Se9vb14+OGHsW3bNqxatSrQ6RARKbjtnojGjcPhQHNzM+bMmYPu7m5ly/jy5csDnBkRkTc2REQ0roqKinDx4kVotVpkZGTg9OnTmDp1aqDTIiLywq/MiIiISHq8qJqIiIikx4aIiIiIpMeGiIiIiKTHhoiIiIikx4aIiIiIpMeGiIiIiKTHhoiIiIikx4aIiIiIpMeGiIiIiKT3P1QBD9BjYUHfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_infer_result(groud_truths, infer_results):\n",
    "    title = 'Boston'\n",
    "    plt.title(title)\n",
    "    x = np.arange(1,20)\n",
    "    y = x\n",
    "    plt.plot(x,y);\n",
    "    plt.xlabel(\"ground truth\")\n",
    "    plt.ylabel(\"infer result\")\n",
    "    plt.scatter(groud_truths,infer_results,color='green',label='training cost')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "draw_infer_result(label,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: -0.1921376038716971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "# Predict and gather true y and predicted y\n",
    "for batch_id, data in enumerate(eval_loader()):\n",
    "    feature = data[0]\n",
    "    label = data[1]\n",
    "    predict = model(feature)\n",
    "\n",
    "    predictions.extend(predict.flatten().numpy())\n",
    "    labels.extend(label.flatten().numpy())\n",
    "\n",
    "# Calculate R2 Score\n",
    "r2 = r2_score(labels, predictions)\n",
    "\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
